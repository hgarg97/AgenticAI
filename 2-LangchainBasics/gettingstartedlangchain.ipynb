{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd2dca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ec1422",
   "metadata": {},
   "source": [
    "## Adding Environments to our Operating System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0292b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Langsmith Tracking and Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "\n",
    "# Necessory to do based on Langchain Documentation\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e975cae",
   "metadata": {},
   "source": [
    "## Using OpenAI models using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b865d3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x0000024E577974D0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000024E57BE0050> root_client=<openai.OpenAI object at 0x0000024E577946E0> root_async_client=<openai.AsyncOpenAI object at 0x0000024E57797CB0> model_name='o1-mini' temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"o1-mini\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ef5a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"What is Agentic AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f4299be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Agentic AI** refers to artificial intelligence systems that possess **agency**, meaning they have the capacity to act independently, make decisions, and pursue goals based on their own objectives rather than solely following predefined instructions or reacting to external inputs. Unlike traditional AI, which is often designed to perform specific tasks or respond to specific stimuli, agentic AI is characterized by its ability to **initiate actions**, **adapt to changing environments**, and **learn from experiences** to achieve long-term objectives.\n",
      "\n",
      "### **Key Characteristics of Agentic AI:**\n",
      "\n",
      "1. **Autonomy:** Agentic AI systems operate with a high degree of independence, making decisions without constant human oversight.\n",
      "2. **Goal-Oriented Behavior:** They set and pursue their own goals, which may align with, complement, or differ from human objectives.\n",
      "3. **Learning and Adaptation:** These systems can learn from interactions and experiences, allowing them to adapt their strategies over time.\n",
      "4. **Decision-Making:** They employ complex reasoning processes to evaluate options and choose the most appropriate actions to achieve their goals.\n",
      "5. **Interaction:** Agentic AI often interacts with other agents (which can be humans or other AI systems) within their environment to accomplish tasks.\n",
      "\n",
      "### **Applications of Agentic AI:**\n",
      "\n",
      "- **Autonomous Vehicles:** Self-driving cars navigate and make real-time decisions to transport passengers safely.\n",
      "- **Personal Assistants:** Advanced virtual assistants that manage schedules, make recommendations, and perform tasks proactively.\n",
      "- **Robotics:** Robots in manufacturing, healthcare, or service industries that can perform complex tasks with minimal human intervention.\n",
      "- **Gaming:** Intelligent non-player characters (NPCs) that exhibit lifelike behaviors and strategies.\n",
      "- **Financial Services:** AI systems that manage investment portfolios, execute trades, and optimize financial strategies.\n",
      "\n",
      "### **Challenges and Considerations:**\n",
      "\n",
      "1. **Ethics and Responsibility:** Determining accountability for the actions taken by agentic AI, especially in scenarios where decisions may have significant consequences.\n",
      "2. **Alignment with Human Values:** Ensuring that the goals and behaviors of agentic AI align with societal norms and ethical standards to prevent harmful outcomes.\n",
      "3. **Control and Oversight:** Developing mechanisms to maintain oversight and control over autonomous AI systems to prevent unintended behaviors.\n",
      "4. **Transparency:** Ensuring that the decision-making processes of agentic AI are understandable and transparent to humans.\n",
      "5. **Security:** Protecting agentic AI systems from malicious activities that could exploit their autonomous capabilities for harmful purposes.\n",
      "\n",
      "### **Future Outlook:**\n",
      "\n",
      "As AI technology continues to advance, the development of agentic AI holds the promise of creating more efficient, adaptable, and intelligent systems capable of handling complex tasks across various industries. However, this progression also necessitates careful consideration of the ethical, societal, and technical challenges to ensure that agentic AI contributes positively to human progress and well-being.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa3f061",
   "metadata": {},
   "source": [
    "## Using Groq Models using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70e428ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"\\n<think>\\nOkay, the user introduced themselves as Kingpin. I need to respond appropriately. First, I should greet them back in a friendly manner. Maybe use their name to make it personal. I should ask how I can assist them today. Keep it open-ended so they can talk about anything they need help with. Let me make sure the tone is warm and welcoming. Let me check for any typos. Alright, that should work.\\n</think>\\n\\nHello, Kingpin! It's great to meet you. How can I assist you today? Feel free to ask about anythingâ€”from trivia and advice to creative ideas or problem-solving. I'm here to help! ðŸ˜Š\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 134, 'prompt_tokens': 17, 'total_tokens': 151, 'completion_time': 0.316375182, 'prompt_time': 0.005458617, 'queue_time': 5.01985499, 'total_time': 0.321833799}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_18a313a21d', 'finish_reason': 'stop', 'logprobs': None}, id='run--1218d780-19ae-4c49-81f0-7c35123e2c43-0', usage_metadata={'input_tokens': 17, 'output_tokens': 134, 'total_tokens': 151})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model = \"qwen-qwq-32b\")\n",
    "model.invoke(\"Hi, my name is Kingpin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee189d9f",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0655e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI engineer. Provide me an answer based on the question\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "041bab73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI engineer. Provide me an answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b28c3a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000024E58286350>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000024E58286D50>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model = \"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f554c7",
   "metadata": {},
   "source": [
    "#### Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61eb1112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI engineer. Provide me an answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000024E58286350>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000024E58286D50>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt|model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54cd04a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI engineer, I'm familiar with Langsmith! It's a powerful tool in the AI development world. \n",
      "\n",
      "**Here's a breakdown:**\n",
      "\n",
      "* **What it is:** Langsmith is an open-source platform designed to simplify the process of building and deploying AI applications, particularly those leveraging large language models (LLMs). \n",
      "\n",
      "* **Key Features:**\n",
      "\n",
      "    * **Streamlined LLMs:** It offers a user-friendly interface for interacting with and fine-tuning various LLMs, making it accessible even to developers without extensive machine learning expertise.\n",
      "\n",
      "    * **Model Deployment:** Langsmith simplifies the deployment of trained models, allowing you to integrate them into your applications or make them accessible via APIs.\n",
      "\n",
      "    * **Data Management:** It provides tools for managing and preparing the data used to train and evaluate your AI models.\n",
      "    * **Experiment Tracking:**  Langsmith helps you track the performance of different model configurations and experiments, streamlining the development process.\n",
      "\n",
      "* **Benefits:**\n",
      "\n",
      "    * **Lower Barrier to Entry:** Makes working with LLMs more accessible to a wider range of developers.\n",
      "\n",
      "    * **Increased Efficiency:** Streamlines the development workflow, saving time and effort.\n",
      "    * **Improved Collaboration:**  Facilitates collaboration among AI developers.\n",
      "\n",
      "* **Who it's for:**\n",
      "\n",
      "    * **Developers:** Those looking to build AI-powered applications using LLMs.\n",
      "    * **Researchers:**  Researchers exploring new applications for LLMs.\n",
      "    * **Data Scientists:**  Data scientists who want to easily deploy and experiment with trained models.\n",
      "\n",
      "**In essence, Langsmith empowers developers to harness the potential of LLMs without getting bogged down in complex technical details.**\n",
      "\n",
      "\n",
      "Let me know if you have any other questions about Langsmith or any other AI topic!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"input\":\"Can you tell me something about Langsmith?\"})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce793c11",
   "metadata": {},
   "source": [
    "### Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26b9e665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## LangChain: Your Toolkit for Building Powerful Language Applications\n",
      "\n",
      "LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). Think of it as a toolbox packed with essential components for building sophisticated and flexible AI applications.\n",
      "\n",
      "**Here's a breakdown of what makes LangChain so special:**\n",
      "\n",
      "**1. Modular and Composability:**\n",
      "\n",
      "LangChain breaks down the complexities of LLM applications into modular components. These components, like prompts, memory, agents, and tools, can be combined and customized to build diverse applications.\n",
      "\n",
      "**2. Chain Creation:**\n",
      "\n",
      "At its core, LangChain excels at creating \"chains,\" which are sequences of these components working together. Imagine a chain where a prompt is used to query an LLM, the result is processed by a tool, and the output is then stored in memory for future use. This modularity allows for complex workflows and powerful functionalities.\n",
      "\n",
      "**3. Memory Management:**\n",
      "\n",
      "LangChain provides various memory mechanisms, enabling your applications to remember past interactions and context. This is crucial for building conversational AI, question-answering systems, and other applications requiring context awareness.\n",
      "\n",
      "**4. Tool Integration:**\n",
      "\n",
      "LangChain seamlessly integrates with external tools, allowing LLMs to access real-world data and functionalities. For example, your LLM could use a calculator tool to perform calculations, a search engine to retrieve information, or a calendar tool to schedule appointments.\n",
      "\n",
      "**5. Agent Functionality:**\n",
      "\n",
      "LangChain empowers you to build autonomous \"agents\" that can interact with their environment, make decisions, and perform tasks. These agents can utilize LLMs for reasoning, planning, and taking actions based on their goals.\n",
      "\n",
      "**6. Extensive Ecosystem:**\n",
      "\n",
      "LangChain boasts a vibrant open-source community and a growing ecosystem of integrations with popular LLMs like OpenAI's GPT models, HuggingFace Transformers, and more.\n",
      "\n",
      "**Use Cases:**\n",
      "\n",
      "LangChain's versatility shines in a wide range of applications:\n",
      "\n",
      "* **Chatbots and Conversational AI:** Create engaging and context-aware chatbots for customer service, education, or entertainment.\n",
      "* **Question Answering Systems:** Build systems that can answer complex questions based on a given context or knowledge base.\n",
      "* **Text Summarization and Generation:** Summarize lengthy documents or generate creative content like stories, poems, or articles.\n",
      "* **Code Generation and Assistance:** Leverage LLMs to assist in coding tasks, generate code snippets, and debug existing code.\n",
      "* **Data Analysis and Insights:** Use LLMs to analyze textual data, extract insights, and generate reports.\n",
      "\n",
      "**Getting Started:**\n",
      "\n",
      "LangChain is relatively easy to learn and use. The official documentation provides comprehensive tutorials, examples, and guidance on getting started.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "LangChain is a powerful and versatile framework that empowers developers to unlock the full potential of LLMs. Its modularity, composability, and extensive ecosystem make it a go-to choice for building innovative and impactful AI applications across various domains.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "response = chain.invoke({\"input\":\"Can you tell me about Langchain?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38e3f867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return a JSON object.\n"
     ]
    }
   ],
   "source": [
    "# Json Output Parser\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dce3a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64f0b3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bccc46d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LLM': 'LLM stands for Large Language Model. It is a type of artificial intelligence (AI) that excels at understanding and generating human language. \\n\\nLLMs are trained on massive datasets of text and code, enabling them to perform a wide range of tasks, including:\\n\\n* **Text Generation:** Writing stories, articles, poems, and more.\\n* **Translation:** Converting text from one language to another.\\n* **Summarization:** Condensing large amounts of text into shorter summaries.\\n* **Question Answering:** Providing answers to questions based on given context.\\n* **Code Generation:** Writing computer code in various programming languages.\\n\\nSome popular examples of LLMs include GPT-3, LaMDA, and BERT.'}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "response = chain.invoke({\"query\": \"What is LLM?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e53690",
   "metadata": {},
   "source": [
    "#### Assignment - Do the JSON, using Chat Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1ac1abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI engineer. Provide me an answer based on the question. Return the response in JSON format\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48107937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Json Output Parser\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser = JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c419548d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'LangChain', 'description': 'LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs).', 'key_features': ['Modular Design:', 'Chain Creation:', 'Memory Management:', 'Prompt Templates:', 'Integration with LLMs:', 'Data Access:', 'Agents:', 'Tools'], 'benefits': ['Streamlines LLM Development:', 'Enhances Application Capabilities:', 'Improves User Experience:', 'Increases Productivity:', 'Promotes Reusability'], 'use_cases': ['Chatbots and Conversational AI:', 'Text Summarization and Generation:', 'Question Answering Systems:', 'Code Generation and Documentation:', 'Data Analysis and Insights:', 'Personalized Learning Experiences'], 'website': 'https://python.langchain.com/'}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "response = chain.invoke({\"input\":\"Can you tell me about Langchain?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b088bbc0",
   "metadata": {},
   "source": [
    "#### For XML Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83acb11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<response><answer>LangChain is a framework for developing applications powered by large language models (LLMs). It provides tools and components to simplify the process of building, chaining, and deploying LLM-based applications.</answer></response> \\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 39, 'total_tokens': 91, 'completion_time': 0.094545455, 'prompt_time': 0.002334638, 'queue_time': 0.07031316, 'total_time': 0.096880093}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--eb0ba438-5291-48a1-8afb-5147d6f16ca0-0' usage_metadata={'input_tokens': 39, 'output_tokens': 52, 'total_tokens': 91}\n"
     ]
    }
   ],
   "source": [
    "##output parser\n",
    "#from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36eb991",
   "metadata": {},
   "source": [
    "### With Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b365abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why couldn't the bicycle find its way home?\",\n",
       " 'punchline': 'Because it lost its bearings!'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3bb72218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': \"Why couldn't the bicycle stand up by itself? Because it was two tired!\"}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Without Pydantic\n",
    "joke_query = \"Tell me a joke .\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb887396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<movie>Splash</movie>\n",
      "<movie>Big</movie>\n",
      "<movie>Forrest Gump</movie>\n",
      "<movie>Apollo 13</movie>\n",
      "<movie>Saving Private Ryan</movie>\n",
      "<movie>The Green Mile</movie>\n",
      "<movie>Cast Away</movie>\n",
      "<movie>Catch Me If You Can</movie>\n",
      "<movie>The Da Vinci Code</movie>\n",
      "<movie>Toy Story (franchise)</movie>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "actor_query = \"Generate the shortened filmography for Tom Hanks.\"\n",
    "\n",
    "output = model.invoke(\n",
    "    f\"\"\"{actor_query}\n",
    "Please enclose the movies in <movie></movie> tags\"\"\"\n",
    ")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89444c6",
   "metadata": {},
   "source": [
    "### Assisgment:\n",
    "Create a simple assistant that uses any LLM and should be pydantic, when we ask about any product it should give you two information product Name, product details tentative price in USD (integer). use chat Prompt Template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73342907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product': 'Shampoo',\n",
       " 'details': 'Brand: XYZ, Type: Moisturizing, Size: 16 oz, Ingredients: Aloe Vera, Coconut Oil, Price: $10',\n",
       " 'price': '$10'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Product(BaseModel):\n",
    "    product_name: str = Field(description=\"name of the product we asked about\")\n",
    "    product_details: str = Field(description=\"small description of the product\")\n",
    "    price: int = Field(description=\"Price of the product in USD\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "query = \"Shampoo\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Product)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI engineer. Provide me the details and price about the product specified by the user. Return the response in JSON format\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268b52df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
