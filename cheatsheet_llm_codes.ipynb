{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e8e69b",
   "metadata": {},
   "source": [
    "# Complete LLM Cheatsheet with RAG and Agents (using Langgraph)\n",
    "\n",
    "- LLM APIs\n",
    "- Huggingface Embedding Models\n",
    "- Data Loaders\n",
    "- Chunking\n",
    "- Vector Embedding using FAISS, Chroma, and Pinecone\n",
    "- Langchain Inbuilt Tools\n",
    "- Creating custom tools\n",
    "- Agentic Orchestration\n",
    "- ReAct Agents\n",
    "- Agentic RAG\n",
    "- MultiAgents (Network and Supervisor)\n",
    "- Human in Loop and Misc. Manipulations with Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a7f01e",
   "metadata": {},
   "source": [
    "## Loading Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6bfb7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13fac10",
   "metadata": {},
   "source": [
    "## LLM APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698f92d8",
   "metadata": {},
   "source": [
    "### OpenAI Env, Model, and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9641b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dfc91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "llm.invoke(\"hello how are you my firend?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3db15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "len(embeddings.embed_query(\"hello how are you my firend?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353929d",
   "metadata": {},
   "source": [
    "### Groq KEY and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc0d73e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8630a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model_name=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "response=llm.invoke(\"what is length of wall of china?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0963e118",
   "metadata": {},
   "source": [
    "### Google Gemini Env, Model, and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf211b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0390feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
    "\n",
    "output = model.invoke(\"hi\")\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3330b60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "embeddings.embed_query(\"Hello AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0249749",
   "metadata": {},
   "source": [
    "## Hugging Face Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c71d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN']=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
    "\n",
    "len(embeddings.embed_query(\"hi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330dd333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "text=\"this is atest documents\"\n",
    "query_result=embeddings.embed_query(text)\n",
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f800701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e437169",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ffd335",
   "metadata": {},
   "source": [
    "### WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# For 1 URL\n",
    "url = ''\n",
    "web_loader=WebBaseLoader(url)\n",
    "data=web_loader.load()\n",
    "\n",
    "# For Multi URL\n",
    "urls = ['', '']\n",
    "docs=[WebBaseLoader(url).load() for url in urls]\n",
    "docs_list=[item for sublist in docs for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf62c2",
   "metadata": {},
   "source": [
    "### TextLoader and DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ceef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "\n",
    "loader=DirectoryLoader(\"../data\",glob=\"./*.txt\",loader_cls=TextLoader)\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d817f9",
   "metadata": {},
   "source": [
    "### PDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc8d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader=PyPDFLoader('syllabus.pdf')\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11c68d",
   "metadata": {},
   "source": [
    "### ArXiv Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "docs = ArxivLoader(query=\"1706.03762\", load_max_docs=2).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d570177",
   "metadata": {},
   "source": [
    "### Wikipedia Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5868bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "docs = WikipediaLoader(query=\"Generative AI\", load_max_docs=4).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39272361",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90c597",
   "metadata": {},
   "source": [
    "### RecursiveCharaterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d012e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Normal Embedding Models\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# OpenAI Embedding Models\n",
    "text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder\n",
    "(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=25\n",
    ")\n",
    "\n",
    "# Common Code\n",
    "doc_splits=text_splitter.split_documents(docs_list)\n",
    "\n",
    "\n",
    "# If only page content needed\n",
    "doc_string=[doc.page_content for doc in doc_splits]\n",
    "\n",
    "# If need to preserve metadata\n",
    "texts = [doc.page_content for doc in doc_splits]\n",
    "metadatas = [doc.metadata for doc in doc_splits]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6e4bb7",
   "metadata": {},
   "source": [
    "### CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f7abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter=CharacterTextSplitter(separator=\"\\n\\n\",chunk_size=100,chunk_overlap=20)\n",
    "text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0173008",
   "metadata": {},
   "source": [
    "### HTMLHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e02bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "    <div>\n",
    "        <h1>Foo</h1>\n",
    "        <p>Some intro text about Foo.</p>\n",
    "        <div>\n",
    "            <h2>Bar main section</h2>\n",
    "            <p>Some intro text about Bar.</p>\n",
    "            <h3>Bar subsection 1</h3>\n",
    "            <p>Some text about the first subtopic of Bar.</p>\n",
    "            <h3>Bar subsection 2</h3>\n",
    "            <p>Some text about the second subtopic of Bar.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h2>Baz</h2>\n",
    "            <p>Some text about Baz</p>\n",
    "        </div>\n",
    "        <br>\n",
    "        <p>Some concluding text about Foo</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on=[\n",
    "    (\"h1\",\"Header 1\"),\n",
    "    (\"h2\",\"Header 2\"),\n",
    "    (\"h3\",\"Header 3\")\n",
    "]\n",
    "\n",
    "html_splitter=HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits=html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547b96f4",
   "metadata": {},
   "source": [
    "### RecursiveJsonSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d4f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "json_splitter=RecursiveJsonSplitter(max_chunk_size=300)\n",
    "json_chunks=json_splitter.split_json(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e7565",
   "metadata": {},
   "source": [
    "## Vector Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c01489",
   "metadata": {},
   "source": [
    "### FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad4427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "# Using Inner Product in FAISS Index\n",
    "\n",
    "index=faiss.IndexFlatIP(3072) # Number of dimensions in the embedding model\n",
    "\n",
    "db = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "# Using Euclidiean Distance in FAISS Index\n",
    "\n",
    "index=faiss.IndexFlatL2(384) # Number of dimensions in the embedding model\n",
    "\n",
    "db = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "# If we just need the docstrings\n",
    "db.add_texts(doc_string)\n",
    "\n",
    "# If we need to add metadata info as well\n",
    "db.add_texts(texts, metadatas=metadatas)\n",
    "\n",
    "# Note: Add texts only works with array of docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a30fa4",
   "metadata": {},
   "source": [
    "### Saving and Loading Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79467d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Index\n",
    "\n",
    "db.save_local(\"saved_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a090255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Index\n",
    "\n",
    "new_vector_store=FAISS.load_local(\n",
    "  \"saved_index\",\n",
    "  embeddings,\n",
    "  allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1318c",
   "metadata": {},
   "source": [
    "### Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore=Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chrome\", # Any Name\n",
    "    embedding=embeddings\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593aefa5",
   "metadata": {},
   "source": [
    "### Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a8f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pinecone_api_key=os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7996c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec  #Serverless: Server will be Managed by the cloud provider\n",
    "\n",
    "pc=Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Index Creation and Loading\n",
    "\n",
    "index_name=\"agentic-ai\"\n",
    "\n",
    "#creating a index\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=768,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(cloud=\"aws\",region=\"us-east-1\")    \n",
    ")\n",
    "\n",
    "#loading the index\n",
    "index=pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13002883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Vector Store and Similarity Search\n",
    "vector_store=PineconeVectorStore(index=index,embedding=embeddings)\n",
    "\n",
    "results = vector_store.similarity_search(\"what is a langchain?\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e99f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store Retriever\n",
    "\n",
    "retriever=vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.7} #hyperparameter\n",
    ")\n",
    "retriever.invoke(\"langchain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc8423",
   "metadata": {},
   "source": [
    "## Langchain Inbuilt Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb92c74",
   "metadata": {},
   "source": [
    "### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b23eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper=WikipediaAPIWrapper(top_k_results=5,doc_content_chars_max= 500)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper= api_wrapper)\n",
    "\n",
    "# To get tool name\n",
    "wiki_tool.name\n",
    "# To get tool description\n",
    "wiki_tool.description\n",
    "# To get tool args\n",
    "wiki_tool.args\n",
    "\n",
    "# Running\n",
    "wiki_tool.run({\"query\": \"elon musk\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46432b85",
   "metadata": {},
   "source": [
    "### Youtube Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4f5a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import YouTubeSearchTool\n",
    "\n",
    "tool = YouTubeSearchTool()\n",
    "\n",
    "# To get tool name\n",
    "tool.name\n",
    "# To get tool description\n",
    "tool.description\n",
    "# To get tool args\n",
    "tool.args\n",
    "\n",
    "# Running\n",
    "tool.run(\"Emergency Awesome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59299f25",
   "metadata": {},
   "source": [
    "### Tavily (Search Engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591921c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TAVILY_API_KEY=os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d382df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tool=TavilySearchResults(tavily_api_key=TAVILY_API_KEY)\n",
    "\n",
    "# Running - 1\n",
    "tool.invoke({\"query\":\"what happend between Trump and Musk today?\"})\n",
    "\n",
    "# Running - 2\n",
    "question = \"what happend between Trump and Musk today?\"\n",
    "complete_query = \"Anwer the follow question by searching the internet and getting best response. Following is the user question: \" + question\n",
    "\n",
    "tool.invoke(complete_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d10122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "tavily_tool=TavilySearch(tavily_api_key=TAVILY_API_KEY)\n",
    "\n",
    "question = \"what happend between Trump and Musk today?\"\n",
    "complete_query = \"Anwer the follow question by searching the internet and getting best response. Following is the user question: \" + question\n",
    "\n",
    "tavily_tool.invoke(complete_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a88ffe0",
   "metadata": {},
   "source": [
    "### DuckDuckGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d8424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "search.invoke(\"what is the latest update on iphone17 release?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e29e3e",
   "metadata": {},
   "source": [
    "### Python REPL Utililty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b180b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run any given python code\n",
    "\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "code = \"\"\"\n",
    "x = 5\n",
    "y = x * 2\n",
    "print(y)\n",
    "\"\"\"\n",
    "\n",
    "repl.run(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be3b43",
   "metadata": {},
   "source": [
    "## Custom Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59822c6",
   "metadata": {},
   "source": [
    "### Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6908bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add two integers.\n",
    "\n",
    "    Args:\n",
    "    a(int): The first integer\n",
    "    b(int): The second integer\n",
    "\n",
    "    Returns:\n",
    "        int: The Sum of a and b\n",
    "    \"\"\"\n",
    "\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e38b8fa",
   "metadata": {},
   "source": [
    "### Subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ffc121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Subtract two integers.\n",
    "\n",
    "    Args:\n",
    "    a(int): The first integer\n",
    "    b(int): The second integer\n",
    "\n",
    "    Returns:\n",
    "        int: The difference of a and b\n",
    "    \"\"\"\n",
    "\n",
    "    return a - b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a1bbda",
   "metadata": {},
   "source": [
    "### Absolute Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f35cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def abs_diff(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Subtract two integers.\n",
    "\n",
    "    Args:\n",
    "    a(int): The first integer\n",
    "    b(int): The second integer\n",
    "\n",
    "    Returns:\n",
    "        int: The absolute difference of a and b\n",
    "    \"\"\"\n",
    "\n",
    "    return abs(a - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5b2318",
   "metadata": {},
   "source": [
    "### Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65aa43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiple(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Multiple two integers.\n",
    "\n",
    "    Args:\n",
    "    a(int): The first integer\n",
    "    b(int): The second integer\n",
    "\n",
    "    Returns:\n",
    "        int: The product of a and b\n",
    "    \"\"\"\n",
    "\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a6e9b",
   "metadata": {},
   "source": [
    "### Divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75d960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Divide two integers.\n",
    "\n",
    "    Args:\n",
    "    a(int): The first integer\n",
    "    b(int): The second integer\n",
    "\n",
    "    Returns:\n",
    "        int: The result of division\n",
    "    \"\"\"\n",
    "\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Denominator cannot be zero.\")\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f9fa7",
   "metadata": {},
   "source": [
    "### Length of Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_word_length(word:str)->int:\n",
    "    \"\"\"\n",
    "    Calculate the length of the word.\n",
    "\n",
    "    Args:\n",
    "    word(str): The word in string\n",
    "\n",
    "    Returns:\n",
    "        int: The length of the word\n",
    "    \"\"\"\n",
    "    return len(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c04654",
   "metadata": {},
   "source": [
    "## Agentic Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3816fa5f",
   "metadata": {},
   "source": [
    "### Pydantic Class for some kind of validation -> used as an Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel , Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "class TopicSelectionParser(BaseModel):\n",
    "    Topic:str=Field(description=\"selected topic\")\n",
    "    Reasoning:str=Field(description='Reasoning behind topic selection')\n",
    "\n",
    "parser=PydanticOutputParser(pydantic_object=TopicSelectionParser)\n",
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b31f5a6",
   "metadata": {},
   "source": [
    "### Custom Agent State Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f824a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc2ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "workflow = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aed967f",
   "metadata": {},
   "source": [
    "### Prebuilt Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16220d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState\n",
    "\n",
    "workflow = StateGraph(MessagesState) \n",
    "# This is the same as our custom defined Agent State Function (right now), if we need something custom, we can use our methods, else MessageState is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6aafff",
   "metadata": {},
   "source": [
    "### Workflow 1: Agentic Orchestration using parser(pydantic class), custom AgentState, and Custom Router Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef0e6d",
   "metadata": {},
   "source": [
    "![alt text](01a93893-3dce-4f90-80d0-b335c9bd36a2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# LLM Supervisor Function using Pydantic Parser, Custom Agent State, Chaining\n",
    "def llm_supervisor_function(state: AgentState):\n",
    "    question = state[\"messages\"][-1]\n",
    "\n",
    "    print(\"Question\", question)\n",
    "\n",
    "    template=\"\"\"\n",
    "    Your task is to classify the given user query into one of the following categories: [USA, Not Related]. \n",
    "    Only respond with the category name and nothing else.\n",
    "\n",
    "    User query: {question}\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"question\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions}\n",
    "    )\n",
    "\n",
    "    chain = prompt | model | parser\n",
    "\n",
    "    response = chain.invoke({\"question\": question})\n",
    "\n",
    "    print(\"Parsed response\", response)\n",
    "\n",
    "    return {\"messages\": [response.Topic]}\n",
    "\n",
    "# Custom Router Function\n",
    "def router_function(state: AgentState):\n",
    "    print(\"-> Router ->\")\n",
    "\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    print(\"last_message: \", last_message)\n",
    "\n",
    "    if \"usa\" in last_message.lower():\n",
    "        return \"RAG Call\"\n",
    "    else:\n",
    "        return \"LLM Call\"\n",
    "    \n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# RAG Function\n",
    "def function2(state: AgentState):\n",
    "    print(\"-> RAG Call ->\")\n",
    "    question = state[\"messages\"][0]\n",
    "    \n",
    "    prompt=PromptTemplate(\n",
    "        template = \"\"\"\n",
    "        You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "        If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\n",
    "        Question: {question} \\n\n",
    "        Context: {context} \\n\n",
    "        Answer:\n",
    "        \"\"\",\n",
    "        \n",
    "        input_variables=['context', 'question']\n",
    "    )\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    result = rag_chain.invoke(question)\n",
    "    return  {\"messages\": [result]}\n",
    "\n",
    "# LLM Function\n",
    "def function3(state: AgentState):\n",
    "    print(\"-> LLM Call ->\")\n",
    "    question = state[\"messages\"][0]\n",
    "    \n",
    "    # Normal LLM call\n",
    "    complete_query = \"Anwer the follow question with you knowledge of the real world. Following is the user question: \" + question\n",
    "    response = model.invoke(complete_query)\n",
    "    return {\"messages\": [response.content]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f5b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_node(\"Supervisor\", llm_supervisor_function)\n",
    "workflow.add_node(\"RAG\", function2)\n",
    "workflow.add_node(\"LLM\", function3)\n",
    "\n",
    "# One Way\n",
    "workflow.set_entry_point(\"Supervisor\")\n",
    "\n",
    "# Other Way\n",
    "workflow.add_edge(START, \"Supervisor\")\n",
    "\n",
    "# When Conditional Edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"Supervisor\",\n",
    "    router_function,\n",
    "    {\n",
    "        \"RAG Call\" : \"RAG\",\n",
    "        \"LLM Call\" : \"LLM\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"RAG\", END)\n",
    "workflow.add_edge(\"LLM\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861717c9",
   "metadata": {},
   "source": [
    "### Workflow 2: Agentic Orchestration Using Message State, Tool Node, Custom Tools, Multi Tool Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b70d9",
   "metadata": {},
   "source": [
    "![alt text](95f33936-ba48-4300-9742-488a7f1cd6f3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d561d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph,MessagesState,START,END\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cabf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Tool\n",
    "\n",
    "@tool\n",
    "def search(query:str):\n",
    "    \"\"\"this is my custom tool for searching a weather\"\"\"\n",
    "    if \"delhi\" in query.lower():\n",
    "        return \"the temp is 45 degree celsius\"\n",
    "    return \"the temp is 25 degree celsius\"\n",
    "\n",
    "# Binding Tool\n",
    "tools = [search]\n",
    "llm_with_tool = llm.bind_tools(tools)\n",
    "\n",
    "response = llm_with_tool.invoke(\"what is weather in delhi?\")\n",
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a94c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Message State\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    question = state['messages']\n",
    "    response = llm_with_tool.invoke(question)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Custom Router Function, which will be replaced by langgraph.prebuilt tools_condition \n",
    "def router_function(state: MessagesState):\n",
    "    message = state[\"messages\"]\n",
    "    last_message = message[-1]\n",
    "\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae2227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tool Node from Tool Node class\n",
    "\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0958a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow2 = StateGraph(MessagesState)\n",
    "workflow2.add_node(\"llmwithtool\",call_model)\n",
    "workflow2.add_node(\"mytools\", tool_node) # The tool Node is used here\n",
    "workflow2.add_edge(START, \"llmwithtool\")\n",
    "workflow2.add_conditional_edges(\"llmwithtool\",\n",
    "                                router_function,\n",
    "                                {\"tools\": \"mytools\",\n",
    "                                END: END})\n",
    "workflow2.add_edge(\"mytools\", \"llmwithtool\") # This edge makes multi tool call\n",
    "app2 = workflow2.compile()\n",
    "app2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e3fde4",
   "metadata": {},
   "source": [
    "### Using tools_condition instead of our custom Router function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using router_function in the add_condition_edges method, use tools_condition. This is a inbuilt function in langgraph which returns \"tools\" (same as in our router function)\n",
    "\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "workflow.add_conditional_edges(\"llmwithtool\",\n",
    "                            tools_condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d53158",
   "metadata": {},
   "source": [
    "### Use of Memory Saver, Stream Messages, and Pretty Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44220118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Only need to add checkpointer to this memory \n",
    "app2 = workflow2.compile(checkpointer=memory)\n",
    "app2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656366a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Creating this events \n",
    "events = app2.stream(\n",
    "    {\"messages\":[\"what is a weather in delhi can you tell me some good hotel for staying in north delhi\"]}, \n",
    "    config=config, \n",
    "    stream_mode=\"values\"\n",
    "    )\n",
    "\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc0a1b",
   "metadata": {},
   "source": [
    "### Viewing Compiled Workflow stored in app variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef35649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3320b661",
   "metadata": {},
   "source": [
    "### Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ff158",
   "metadata": {},
   "source": [
    "![alt text](004a619e-f184-41ff-b629-ecdad04b9eb0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0cffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Retrieval Step until Vector Embedding creation and adding the doc list to it\n",
    "\n",
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0873d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Adding retriever as a tool\n",
    "retriever_tool=create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retriever_blog_post\", # Any Name\n",
    "    \"vector database data description, comprehensive for the model to make sense of\", # Description for vector db data\n",
    "    )\n",
    "\n",
    "# Adding to tools and Tool Node Creation\n",
    "tools=[retriever_tool]\n",
    "llm_with_tool=llm.bind_tools(tools)\n",
    "\n",
    "retriever_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94695de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated,Sequence, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "# Pydantic Class \n",
    "class grade(BaseModel):\n",
    "    binary_score:str=Field(description=\"Relvance score 'yes' or 'no'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c692937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "from langchain import hub\n",
    "from typing import Literal\n",
    "#we use it for type of hinting\n",
    "\n",
    "def LLM_Decision_Maker(state:AgentState):\n",
    "    print(\"----CALL LLM_DECISION_MAKE----\")\n",
    "    message=state[\"messages\"]\n",
    "    last_message=message[-1]\n",
    "    question=last_message.content\n",
    "    response=llm_with_tool.invoke(question)\n",
    "    return {\"messages\":[response]}\n",
    "\n",
    "\n",
    "def grade_documents(state:AgentState)->Literal[\"Output Generator\", \"Query Rewriter\"]:\n",
    "    print(\"----CALLING GRADE FOR CHECKING RELEVANCY----\")\n",
    "    llm_with_structure_op=llm.with_structured_output(grade) # To provide response in a certain assigned schema (grade here)\n",
    "    \n",
    "    prompt=PromptTemplate(\n",
    "        template=\"\"\"You are a grader deciding if a document is relevant to a user’s question.\n",
    "                    Here is the document: {context}\n",
    "                    Here is the user’s question: {question}\n",
    "                    If the document talks about or contains information related to the user’s question, mark it as relevant. \n",
    "                    Give a 'yes' or 'no' answer to show if the document is relevant to the question.\"\"\",\n",
    "                    input_variables=[\"context\", \"question\"]\n",
    "                    )\n",
    "     \n",
    "    chain=prompt|llm_with_structure_op\n",
    "     \n",
    "     \n",
    "    message=state['messages']\n",
    "    \n",
    "    last_message = message[-1]\n",
    "    \n",
    "    question = message[0].content\n",
    "    \n",
    "    docs = last_message.content\n",
    "    \n",
    "    scored_result=chain.invoke({\"question\": question, \"context\": docs})\n",
    "    \n",
    "    score=scored_result.binary_score\n",
    "     \n",
    "    if score==\"yes\":\n",
    "        print(\"----DECISION: DOCS ARE RELEVANT----\")\n",
    "        return \"generator\"\n",
    "    else:\n",
    "        print(\"----DECISION: DOCS ARE NOT RELEVANT----\")\n",
    "        return \"rewriter\"\n",
    "    \n",
    "\n",
    "def generate(state:AgentState):\n",
    "    print(\"----RAG OUTPUT GENERATE----\")\n",
    "    \n",
    "    message=state[\"messages\"]\n",
    "    question=message[0].content\n",
    "    \n",
    "    last_message = message[-1]\n",
    "    docs = last_message.content\n",
    "\n",
    "    # To assign a said prompt for the RAG part of code using the preexisting library\n",
    "    prompt=hub.pull(\"rlm/rag-prompt\")\n",
    "    \n",
    "    rag_chain=prompt | llm\n",
    "    \n",
    "    response=rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    \n",
    "    print(f\"this is my response:{response}\")\n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def rewrite(state:AgentState):\n",
    "    print(\"----TRANSFORM QUERY----\")\n",
    "    message=state[\"messages\"]\n",
    "    \n",
    "    question=message[0].content\n",
    "    \n",
    "    input= [HumanMessage(content=f\"\"\"Look at the input and try to reason about the underlying semantic intent or meaning. \n",
    "                    Here is the initial question: {question} \n",
    "                    Formulate an improved question: \"\"\")\n",
    "       ]\n",
    "\n",
    "    response=llm.invoke(input)\n",
    "    \n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdaa757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agentic Orchestration\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "workflow=StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"LLM Decision Maker\",LLM_Decision_Maker)\n",
    "workflow.add_node(\"Vector Retriever\",retriever_node)\n",
    "workflow.add_node(\"Output Generator\",generate)\n",
    "workflow.add_node(\"Query Rewriter\",rewrite)\n",
    "\n",
    "workflow.add_edge(START,\"LLM Decision Maker\")\n",
    "workflow.add_conditional_edges(\"LLM Decision Maker\",\n",
    "                               tools_condition,\n",
    "                               {\"tools\":\"Vector Retriever\",\n",
    "                                END:END\n",
    "                                })\n",
    "workflow.add_conditional_edges(\"Vector Retriever\",\n",
    "                               grade_documents,\n",
    "                               {\"generator\":\"Output Generator\",\n",
    "                                \"rewriter\":\"Query Rewriter\"\n",
    "                                })\n",
    "workflow.add_edge(\"Output Generator\",END)\n",
    "workflow.add_edge(\"Query Rewriter\",\"LLM Decision Maker\")\n",
    "\n",
    "app=workflow.compile()\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2b2c6",
   "metadata": {},
   "source": [
    "### Invoking created Agentic Apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b70263",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.invoke({\"messages\":[\"what is LLM Powered Autonomous Agents explain the planning and reflection and prompt engineering explain me in terms of agents and langchain?\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc603c2",
   "metadata": {},
   "source": [
    "## MultiAgents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3f1afb",
   "metadata": {},
   "source": [
    "#### Concept of Command in MultiAgents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c7c9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition is 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Command(update={'sum': 30}, goto='multiply')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.types import Command\n",
    "\n",
    "def add_number(state):\n",
    "    result = state[\"num1\"] + state[\"num2\"]\n",
    "    print(f\"Addition is {result}\")\n",
    "\n",
    "    return Command(goto=\"multiply\", update = {\"sum\": result}) # Updates the current state, and tells which Agent to go to Next\n",
    "\n",
    "state = {\"num1\": 10, \"num2\": 20}\n",
    "\n",
    "add_number(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c8a88",
   "metadata": {},
   "source": [
    "#### Concept of create_react_agent to use inbuilt library to create an agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dae8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Creates an Agent, with defined LLM, tools, and the PROMPT we provide\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "        llm,\n",
    "        tools = [tools_arr],\n",
    "        prompt = \"PROMPT FOR THE AGENT\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3561c9d",
   "metadata": {},
   "source": [
    "### Network/Collaboration MultiAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111cdfc",
   "metadata": {},
   "source": [
    "![alt text](b9c5dea6-b44a-4c74-a92f-9d2642bbd17b.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3420e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "from typing import Annotated\n",
    "from langgraph.types import Command\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import MessagesState,StateGraph, START,END\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ea2e3",
   "metadata": {},
   "source": [
    "#### Tools and Function Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da31808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python REPL Tool Creation to run the code it recieves\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "    ):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "        \n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    \n",
    "    result_str = f\"Successfully executed:\\n\\`\\`\\`python\\n{code}\\n\\`\\`\\`\\nStdout: {result}\"\n",
    "    return (\n",
    "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# A system prompt to tell LLM, adding agent vise context\n",
    "def make_system_prompt(instruction:str)->str:\n",
    "    return  (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "        \" will help where you left off. Execute what you can to make progress.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "        f\"\\n{instruction}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Directing the LLM, what to do based on the last_message\n",
    "def get_next_node(last_message: BaseMessage, goto: str):\n",
    "    if \"FINAL ANSWER\" in last_message:\n",
    "        return END\n",
    "    \n",
    "    return goto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a447e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 1 -> Researcher Node\n",
    "\n",
    "def research_node(state: MessagesState)-> Command[Literal[\"chart_generator\", END]]:\n",
    "    research_agent = create_react_agent(\n",
    "        llm,\n",
    "        tools = [search_tool],\n",
    "        prompt = make_system_prompt(\n",
    "            \"You can only do research. You are working with a chart generator colleague.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    result = research_agent.invoke(state)\n",
    "\n",
    "    last_message = result[\"messages\"][-1]\n",
    "\n",
    "    goto=get_next_node(last_message,\"chart_generator\")\n",
    "\n",
    "    result[\"messages\"][-1] = HumanMessage(content=result[\"messages\"][-1].content, name=\"researcher\")\n",
    "\n",
    "    return Command(update= {\"messages\": result[\"messages\"]}, goto = goto)\n",
    "\n",
    "\n",
    "# Agent 2 -> Chart Creation Node\n",
    "\n",
    "def chart_node(state: MessagesState)-> Command[Literal[\"researcher\", END]]:\n",
    "    chart_agent = create_react_agent(\n",
    "        llm,\n",
    "        tools = [python_repl_tool],\n",
    "        prompt = make_system_prompt(\n",
    "            \"You can only generate charts. You are working with a researcher colleague.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    result = chart_agent.invoke(state)\n",
    "\n",
    "    last_message = result[\"messages\"][-1]\n",
    "\n",
    "    goto=get_next_node(last_message,\"researcher\")\n",
    "\n",
    "    result[\"messages\"][-1] = HumanMessage(content=result[\"messages\"][-1].content, name=\"chart_generator\")\n",
    "\n",
    "    return Command(update= {\"messages\": result[\"messages\"]}, goto= goto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e6418b",
   "metadata": {},
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ed8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"researcher\", research_node)\n",
    "workflow.add_node(\"chart_generator\", chart_node)\n",
    "\n",
    "workflow.add_edge(START, \"researcher\")\n",
    "app = workflow.compile()\n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af30fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.invoke({\"messages\": [(\"user\",\"get the UK's GDP over the past 3 years, then make a line chart of it.Once you make the chart, finish.\")],})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d77e476",
   "metadata": {},
   "source": [
    "### Supervisor MultiAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27988e9",
   "metadata": {},
   "source": [
    "![alt text](a3a4db77-825b-49e9-abdc-ff0bb7255e37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e52e78",
   "metadata": {},
   "source": [
    "#### Tools and Function Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a81a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from typing import Annotated\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3045ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool 1 - Researcher\n",
    "TAVILY_API_KEY=os.getenv(\"TAVILY_API_KEY\")\n",
    "search_tool=TavilySearchResults(tavily_api_key=TAVILY_API_KEY)\n",
    "\n",
    "\n",
    "# Tool 2 - Coder\n",
    "repl=PythonREPL()\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(code: Annotated[str, \"The python code to execute to generate your chart.\"]):\n",
    "    \"\"\"Use this to execute python code and do math. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    \n",
    "    result_str = f\"Successfully executed:\\n\\`\\`\\`python\\n{code}\\n\\`\\`\\`\\nStdout: {result}\"\n",
    "    return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c34550",
   "metadata": {},
   "outputs": [],
   "source": [
    "members=[\"researcher\",\"coder\"]\n",
    "\n",
    "options = members+[\"FINISH\"]\n",
    "\n",
    "class Router(TypedDict):\n",
    "    next: Literal['researcher', 'coder', 'FINISH']\n",
    "\n",
    "#### IMPORTANT\n",
    "class State(MessagesState):\n",
    "    next: str\n",
    "\n",
    "# State will look like this\n",
    "# state={\"messages\": [\"hi\"], \"next\": \"research_agent\"}\n",
    "\n",
    "system_prompt = f\"\"\"\"\n",
    "You are a supervisor, tasked with managing a conversation between the following workers: {members}. \n",
    "Given the following user request, respond with the worker to act next. \n",
    "Each worker will perform a task and respond with their results and status. \n",
    "When finished, respond with FINISH.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba27d686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGENT 1 [SUPERVISOR]\n",
    "\n",
    "def supervisor_agent(state: State)->Command[Literal['researcher', 'coder', '__end__']]:\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}, ] + state[\"messages\"]\n",
    "\n",
    "    llm_with_structured_output = llm.with_structured_output(Router)\n",
    "\n",
    "    response = llm_with_structured_output.invoke(messages)\n",
    "\n",
    "    # this is my next worker agent\n",
    "    goto = response[\"next\"]\n",
    "\n",
    "    print(\"***************BELOW IS MY GOTO****************\")\n",
    "\n",
    "    print(goto)\n",
    "\n",
    "    if goto == \"FINISH\":\n",
    "        return '__end__'\n",
    "    \n",
    "    return Command(goto= goto, update={\"next\": goto})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a2cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGENT 2 [RESEARCHER]\n",
    "\n",
    "def researcher_agent(state: State)->Command[Literal['supervisor']]:\n",
    "    research_agent = create_react_agent(llm, tools=[search_tool], prompt=\"You are a researcher. DO NOT do any math.\")\n",
    "\n",
    "    result = research_agent.invoke(state)\n",
    "\n",
    "    return Command(\n",
    "        update = {\n",
    "            \"messages\": [\n",
    "                HumanMessage(content = result[\"messages\"][-1].content, name=\"researcher\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6808bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGENT 3 [CODER]\n",
    "\n",
    "def coder_agent(state: State)->Command[Literal['supervisor']]:\n",
    "    code_agent = create_react_agent(llm, tools=[python_repl_tool], prompt=\"You are a coder. DO NOT do any research.\")\n",
    "\n",
    "    result = code_agent.invoke(state)\n",
    "\n",
    "    return Command(\n",
    "        update = {\n",
    "            \"messages\": [\n",
    "                HumanMessage(content = result[\"messages\"][-1].content, name = \"coder\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0504c771",
   "metadata": {},
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed37d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(State)\n",
    "\n",
    "graph.add_node(\"supervisor\", supervisor_agent)\n",
    "graph.add_node(\"researcher\", researcher_agent)\n",
    "graph.add_node(\"coder\", coder_agent)\n",
    "\n",
    "graph.add_edge(START, \"supervisor\")\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb453d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in app.stream({\"messages\": [(\"user\", \"What's the square root of 49?\")]}, subgraphs=True):\n",
    "    print(s)\n",
    "    print(\"**********BELOW IS MY STATE***************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a785b367",
   "metadata": {},
   "source": [
    "## Human In Loop and Misc. Agent Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865ce77",
   "metadata": {},
   "source": [
    "#### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9dc4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb780a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(x: int, y: int) -> int:\n",
    "    \"\"\"Multiplies two numbers.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "@tool\n",
    "def search(query: str):\n",
    "    \"\"\"search the web for a query and return the results\"\"\"\n",
    "    tavily=TavilySearchResults()\n",
    "    result=tavily.invoke(query)\n",
    "    return f\"Result for {query} is: \\n{result}\"\n",
    "\n",
    "\n",
    "tools = [multiply, search]\n",
    "llm_with_tools=llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfafea75",
   "metadata": {},
   "source": [
    "#### New Concept: Handling Tools and Understanding Tool Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32e5c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=llm_with_tools.invoke(\"what is current gdp of india?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32b2b5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'search',\n",
       "  'args': {'query': 'current GDP of India'},\n",
       "  'id': 't8jg5wds2',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a6f8b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool_calls[0][\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5300a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'current GDP of India'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool_calls[0][\"args\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18393b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multiply': StructuredTool(name='multiply', description='Multiplies two numbers.', args_schema=<class 'langchain_core.utils.pydantic.multiply'>, func=<function multiply at 0x000002331E986700>),\n",
       " 'search': StructuredTool(name='search', description='search the web for a query and return the results', args_schema=<class 'langchain_core.utils.pydantic.search'>, func=<function search at 0x000002331E986660>)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tool Mapping\n",
    "\n",
    "tool_mapping={tool.name:tool for tool in tools}\n",
    "tool_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2df4045e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='search', description='search the web for a query and return the results', args_schema=<class 'langchain_core.utils.pydantic.search'>, func=<function search at 0x000002331E986660>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_mapping[\"search\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3756f1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harshit\\AppData\\Local\\Temp\\ipykernel_16552\\1333534694.py:9: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily=TavilySearchResults()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Result for What is the capital of india? is: \\n[{\\'title\\': \\'New Delhi - Wikipedia\\', \\'url\\': \\'https://en.wikipedia.org/wiki/New_Delhi\\', \\'content\\': \\'Appearance\\\\n\\\\nmove to sidebar hide\\\\n\\\\nCoordinates: 28°36′50″N 77°12′32″E / 28.61389°N 77.20889°E / 28.61389; 77.20889\\\\n\\\\nImage 4: Page semi-protected\\\\n\\\\nFrom Wikipedia, the free encyclopedia\\\\n\\\\nCapital city of India\\\\n\\\\nThis article is about the capital of India, within the union territory of Delhi. For other uses, see New Delhi (disambiguation) \"New Delhi (disambiguation)\"). [...] New Delhi (/ˈ nj uː ˈ d ɛ.l i/ⓘ;( _\\\\\\\\_\\\\\\\\\\\\\\\\_Naī Dillī\\\\\\\\\\\\\\\\_\\\\\\\\__, pronounced( \"Help:IPA/Hindi and Urdu\")) is the capital of India and a part of the National Capital Territory of Delhi (NCT). New Delhi is the seat of all three branches of the Government of India, hosting the Rashtrapati Bhavan, Sansad Bhavan, and the Supreme Court. New Delhi is a municipality within the NCT, administered by the New Delhi Municipal Council (NDMC), which covers mostly Lutyens\\\\\\' Delhi and a few adjacent areas. The municipal [...] a Hindu Mandir  \\\\n   Image 30: Qila-i-Kuhna Mosque inside Old Fort, a mosque Qila-i-Kuhna Mosque inside Old Fort, \\\\n\\\\na mosque \\\\n\\\\nGovernment\\\\n----------\\\\n\\\\nMain articles: New Delhi Municipal Council, Government of Delhi, and Delhi Police\\\\n\\\\nThe national capital of India, New Delhi is jointly administered by both the Central Government of India and the local Government of Delhi, it is also the capital of the National Capital Territory (NCT) of Delhi.\\\\n\\\\nImage 31\\\\n\\\\nMunicipalities of Delhi\\\\n\\\\nImage 32\\', \\'score\\': 0.9108078}, {\\'title\\': \\'Capital of India - Definition, Meaning & Synonyms - Vocabulary.com\\', \\'url\\': \\'https://www.vocabulary.com/dictionary/capital%20of%20India\\', \\'content\\': \\'1.  noun\\\\n    \\\\n    the capital of India is a division of the old city of Delhi\\\\n    \\\\n    synonyms: Indian capital, New Delhi\\\\n    \\\\n    see moresee less\\\\n    \\\\n    example of:\\\\n    \\\\n    national capital\\\\n    \\\\n    the capital city of a nation\\\\n    \\\\n\\\\nCite this entry\\\\n\\\\nStyle:\\\\n\\\\nMLA\\\\n\\\\n   MLA\\\\n   APA\\\\n   Chicago\\\\n\\\\n\"Capital of India.\" _Vocabulary.com Dictionary,_ Vocabulary.com,  of India. Accessed 11 May. 2025.\\\\n\\\\nCopy citation\\', \\'score\\': 0.90810597}, {\\'title\\': \\'New Delhi | History, Population, Map, & Facts - Britannica\\', \\'url\\': \\'https://www.britannica.com/place/New-Delhi\\', \\'content\\': \\'Image 19: New Delhi, India\\\\n\\\\nNew Delhi, India(more)\\\\n\\\\nNew Delhi, national capital of India. It is situated in the north-central part of the country on the west bank of the Yamuna River, adjacent to and just south of Delhi city (Old Delhi) and within the Delhi national capital territory. [...] Delhi, city and national capital, and union territory, north-central India. The city of Delhi actually consists of two components: Old Delhi, in the north, the historic city; and New Delhi, in the south, since 1947 the capital of India, built in the first part of the 20th century as the capital of British India.\\\\n\\\\nImage 45: Rashtrapati Bhavan and the Jaipur Column [...] What is New Delhi?\\\\n\\\\nNew Delhi is the national capital of India.\\\\n\\\\n### \\\\n\\\\nWhere is New Delhi?\\\\n\\\\nNew Delhi is situated in the north-central part of India on the west bank of the Yamuna River, adjacent to and just south of Old Delhi, the historic centre of Delhi, and within the Delhi national capital territory.\\\\n\\\\n### \\\\n\\\\nWhen was New Delhi founded?\\', \\'score\\': 0.84215075}, {\\'title\\': \\'Capital City of India] - CountryReports\\', \\'url\\': \\'https://www.countryreports.org/country/India/capital-city.htm\\', \\'content\\': \\'|  |  |\\\\n| --- | --- |\\\\n| Capital City | New Delhi |\\\\n| Capital location | 28 36 N, 77 12 E | [...] of the latter designated as New Delhi; the new capital was not formally inaugurated until 1931. | [...] | Capital - history | the city\\\\\\'s name is associated with various myths and legends; the original name for the city may have been Dhilli or Dhillika; alternatively, the name could be a corruption of the Hindustani words \"dehleez\" or \"dehali\" - both terms meaning \"threshold\" or \"gateway\" - and indicative of the city as a gateway to the Gangetic Plain; after the British decided to move the capital of their Indian Empire from Calcutta to Delhi in 1911, they created a new governmental district south\\', \\'score\\': 0.82254606}, {\\'title\\': \"India\\'s Capital Is Falling Apart - YouTube\", \\'url\\': \\'https://www.youtube.com/watch?v=mXqXGHs1Fp0\\', \\'content\\': \"India\\'s capital isn’t just New Delhi — it\\'s a sprawling, chaotic megacity known as the National Capital Region (NCR), home to over 30 million people. But this region isn’t governed like one city. It’s a patchwork of rival cities — Delhi, Gurugram, Noida, Ghaziabad, Faridabad — all fighting for power, but sharing the same roads, air, and problems. From unplanned urban growth, traffic gridlock, and toxic pollution, to a metro system that doesn’t fully connect and water wars between states, this [...] Most countries have just one capital and so does India kind of. It\\'s complicated and chaotic. Delhi isn\\'t just a city. It\\'s a region, a puzzle, a battleground of cities within cities that don\\'t always play nice. One\\'s the political capital, another\\'s a tech hub. A third is a corporate beast. And none of them fully agree on who\\'s in charge. Welcome to the national capital region. Delhi, Noida, Gurugram, Gazabad to name a few. A sprawling engine of 30 million souls stitched together by tangled [...] borders, feuding governments, and a thousand ambitions colliding like particles in a super collider. It\\'s one of the largest urban regions on Earth, bigger than Australia\\'s entire population, but governed like a group project where nobody wants to do the homework. But how did we get here? Why is Delhi the capital in the first place? Because of the Brits. Surprise, surprise. In 1911, the UK decided to move the capital from Kolkata to Delhi. Partly for symbolism, partly for geography. Delhi was\", \\'score\\': 0.81418616}]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_mapping[\"search\"].invoke({\"query\":\"What is the capital of india?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f94e4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Result for current GDP of India is: \\n[{\\'title\\': \\'World GDP Ranking 2025 List - ClearTax\\', \\'url\\': \\'https://cleartax.in/s/world-gdp-ranking-list\\', \\'content\\': \"India is currently ranked as the 4th largest economy globally in 2025 as of July 2025, overtaking Japan to secure the 4th position among the world\\'s top 10 largest economies, with a nominal GDP of $4.19 trillion in 2025. Moreover, the IMF forecasts that by 2028, India will overtake Germany to become the 3rd largest economy worldwide. [...] As per IMF projections, India\\'s GDP grow is at 6.2% in 2024-25 and 2025-26.With an estimated real GDP of Rs. 187.95 lakh crore in 2024-25, against the real GDP of Rs. 176.51 in 2023-24 generated by a population of over 1 billion, India is among the highest population-based economies in the world. India’s nominal GDP has grown 105% in just a decade, which means that it has more than doubled from 2014 to 2025. [...] In 2025, India has become a $4 trillion economy. As per IMF Data, India is the 4th largest economy in the world in the year 2025 on par with Japan. Today,India’s leading economic contributors are traditional and modern agriculture, technology services, the handicraft industry, and business outsourcing.\", \\'score\\': 0.8539252}, {\\'title\\': \\'India GDP - Trading Economics\\', \\'url\\': \\'https://tradingeconomics.com/india/gdp\\', \\'content\\': \\'##### Members\\\\n\\\\n##### \\\\n\\\\n# India GDP\\\\n\\\\n## The Gross Domestic Product (GDP) in India was worth 3912.69 billion US dollars in 2024, according to official data from the World Bank. The GDP value of India represents 3.69 percent of the world economy. source: World Bank [...] ## GDP in India averaged 834.50 USD Billion from 1960 until 2024, reaching an all time high of 3912.69 USD Billion in 2024 and a record low of 37.03 USD Billion in 1960. This page provides the latest reported value for - India GDP - plus previous releases, historical high and low, short-term forecast and long-term prediction, economic calendar, survey consensus and news. India GDP - values, historical data and charts - was last updated on August of 2025. [...] | Related | Last | Previous | Unit | Reference |\\\\n| --- | --- | --- | --- | --- |\\\\n| Fiscal Year GDP Growth | 6.50 | 9.20 | percent | Mar 2025 |\\\\n| GDP | 3912.69 | 3638.49 | USD Billion | Dec 2024 |\\\\n| GDP Growth Rate YoY | 7.40 | 6.40 | percent | Mar 2025 |\\\\n| GDP Growth Rate | 2.00 | 1.90 | percent | Mar 2025 |\\\\n| GDP per Capita | 2396.71 | 2270.91 | USD | Dec 2024 |\\\\n| GDP per Capita PPP | 9817.07 | 9301.76 | USD | Dec 2024 |\\', \\'score\\': 0.8455478}, {\\'title\\': \\'India Fiscal Year GDP Growth - Trading Economics\\', \\'url\\': \\'https://tradingeconomics.com/india/full-year-gdp-growth\\', \\'content\\': \\'| Related | Last | Previous | Unit | Reference |\\\\n| --- | --- | --- | --- | --- |\\\\n| Fiscal Year GDP Growth | 6.50 | 9.20 | percent | Mar 2025 |\\\\n| GDP | 3912.69 | 3638.49 | USD Billion | Dec 2024 |\\\\n| GDP Growth Rate YoY | 7.40 | 6.40 | percent | Mar 2025 |\\\\n| GDP Constant Prices | 51351.63 | 47265.42 | INR Billion | Mar 2025 |\\\\n| GDP from Agriculture | 6773.89 | 7757.32 | INR Billion | Mar 2025 |\\\\n| GDP from Construction | 4636.41 | 3899.90 | INR Billion | Mar 2025 | [...] | GDP from Manufacturing | 8299.55 | 6960.49 | INR Billion | Mar 2025 |\\\\n| GDP from Mining | 1013.49 | 824.88 | INR Billion | Mar 2025 |\\\\n| GDP from Public Administration | 5623.82 | 5544.70 | INR Billion | Mar 2025 |\\\\n| GDP from Utilities | 1000.44 | 963.01 | INR Billion | Mar 2025 |\\\\n| GDP Growth Rate | 2.00 | 1.90 | percent | Mar 2025 |\\\\n| Gross Fixed Capital Formation | 17411.50 | 14991.34 | INR Billion | Mar 2025 | [...] ## Full Year GDP Growth in India decreased to 6.50 percent in 2025 from 9.20 percent in 2024. Full Year GDP Growth in India averaged 6.42 percent from 2006 until 2025, reaching an all time high of 9.70 percent in 2022 and a record low of -5.80 percent in 2021. This page includes a chart with historical data for India Full Year GDP Growth. India Fiscal Year GDP Growth - data, historical chart, forecasts and calendar of releases - was last updated on August of 2025.\\', \\'score\\': 0.79901963}, {\\'title\\': \"India\\'s Economic Juggernaut On Way To Becoming The 3rd Largest ...\", \\'url\\': \\'https://www.forbes.com/sites/sarwantsingh/2025/07/14/indias-economic-juggernaut-on-way-to-becoming-the-3rd-largest-economy/\\', \\'content\\': \"Despite external pressures, India\\'s economy continues to demonstrate impressive resilience. In nominal terms, the country is projected to become a USD 30 trillion economy by 2047, targeting an average nominal GDP growth rate of 9%–10% per year. India’s gross savings to GDP ratio, too, is expected to improve, projected to reach 48% by 2036–37 under a linear trend scenario, supporting strong investment-led growth. Though the country faces some concerns regarding the current account balance and [...] India is already on the path to economic greatness, having surpassed Japan recently to become the 4th largest economy in 2025.And that’s just the beginning! By 2028, India is also expected to become the world’s third-largest economy, moving ahead of Germany. India is aiming to add about USD 1 trillion to its GDP every 12 to 18 months over the next decade, targeting a CAGR of 9% nominal GDP growth rate from 2025 to 2047. While this is undoubtedly ambitious, India’s strong macroeconomic [...] Maharashtra, Gujarat, and Tamil Nadu are on track to reach a GDP of USD 1 trillion each by 2035—collectively matching the combined economic might of the United Arab Emirates, Sweden, and Belgium, and highlighting their role as the engines of India’s economic progress. Rapid expansion, urban development, and a focus on high-value industries have transformed these states into manufacturing powerhouses and financial centers, helping India gain a competitive edge and avoiding over-reliance on any\", \\'score\\': 0.7171114}, {\\'title\\': \\'India to remain fastest growing economy in 2025 & 2026, while ...\\', \\'url\\': \\'https://m.economictimes.com/news/economy/indicators/india-to-remain-fastest-growing-economy-in-2025-2026-while-global-growth-to-decline-morgan-stanley/articleshow/122172107.cms\\', \\'content\\': \"Morgan Stanley\\'s latest report forecasts India to maintain its position as the fastest-growing economy, projecting real GDP growth at 5.9% in 2025 and 6.4% in 2026. Despite a predicted global economic slowdown, with growth declining from 3.5% in 2024 to 2.5% in 2025, India is expected to outperform.\\\\n\\\\nIndia to remain fastest growing economy in 2025 & 2026, while global growth to decline: Morgan Stanley\\\\nET logo\\\\n\\\\n### Live Events\", \\'score\\': 0.51105833}]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_mapping[result.tool_calls[0][\"name\"]].invoke(result.tool_calls[0][\"args\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef9a270",
   "metadata": {},
   "source": [
    "#### Human In Loop Use Case 1: Taking Permission and Proceeding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c5017",
   "metadata": {},
   "source": [
    "![alt text](428a6a26-e9b8-4125-b8a9-aab14b692d23.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bb95c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Sequence, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import StateGraph, START,END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"State for the agent.\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage],operator.add]\n",
    "\n",
    "# LLM Calling Function\n",
    "def invoke_model(state:AgentState):\n",
    "    messages=state[\"messages\"]\n",
    "    question=messages[-1]\n",
    "    response=llm_with_tools.invoke(question)\n",
    "    return {\"messages\":[response]}\n",
    "\n",
    "# Router Function\n",
    "def router(state:AgentState):\n",
    "    tool_calls=state[\"messages\"][-1].tool_calls\n",
    "    if len(tool_calls)>0:\n",
    "        return \"tool\" #key name\n",
    "    else:\n",
    "        return \"end\" #key name\n",
    "    \n",
    "# Tool Calling Function: Responsible for Asking User for approval to use a tool\n",
    "def invoke_tool(state:AgentState):\n",
    "    tool_details=state[\"messages\"][-1].tool_calls\n",
    "    \n",
    "    if tool_details is None:\n",
    "        return Exception(\"No tool calls found in the last message.\")\n",
    "    \n",
    "    print(f\"Seleted tool: {tool_details[0]['name']}\")\n",
    "    \n",
    "    if tool_details[0][\"name\"]==\"search\":\n",
    "        response=input(prompt=f\"[yes/no] do you want to continue with this expensive web search\")\n",
    "        if response.lower()==\"no\":\n",
    "            print(\"web search discarded by the user. exiting gracefully\")\n",
    "            raise Exception(\"Web search discarded by the user.\")\n",
    "            \n",
    "    \n",
    "    response=tool_mapping[tool_details[0][\"name\"]].invoke(tool_details[0][\"args\"])\n",
    "    return {\"messages\":[response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6aebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow\n",
    "\n",
    "graph=StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"ai_assistant\", invoke_model)\n",
    "\n",
    "# eariler we were using the ToolNode(from the prebuilt library) from list of tool\n",
    "# but now we have created tool_invoke (custom funtion)\n",
    "# why we are doing it? -> as a user if we want to take a authority to which i need to give permission for execution\n",
    "\n",
    "graph.add_node(\"tool\", invoke_tool)\n",
    "\n",
    "graph.add_conditional_edges(\"ai_assistant\",\n",
    "                            router,\n",
    "                            {\n",
    "                                \"tool\":\"tool\", ##with the key tool which value is associated <tool>\n",
    "                                \"end\":END\n",
    "                            }\n",
    "                            )\n",
    "\n",
    "graph.add_edge(\"tool\", END)\n",
    "\n",
    "graph.set_entry_point(\"ai_assistant\")\n",
    "\n",
    "app=graph.compile()\n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e7940",
   "metadata": {},
   "source": [
    "### Human In Loop Use Case 2: INTERRUPT BEFORE (Take User Input and Integrate in Workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06bf5c",
   "metadata": {},
   "source": [
    "![alt text](18bf5d8f-0193-4d25-ae43-8980b58d2220.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9a574c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_node=ToolNode(tools)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
