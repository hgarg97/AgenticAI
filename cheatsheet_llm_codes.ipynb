{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a7f01e",
   "metadata": {},
   "source": [
    "## Loading Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bfb7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13fac10",
   "metadata": {},
   "source": [
    "## LLM APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698f92d8",
   "metadata": {},
   "source": [
    "### OpenAI Env, Model, and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9641b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dfc91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "llm.invoke(\"hello how are you my firend?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3db15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "len(embeddings.embed_query(\"hello how are you my firend?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353929d",
   "metadata": {},
   "source": [
    "### Groq KEY and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d73e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8630a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model_name=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "response=llm.invoke(\"what is length of wall of china?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0963e118",
   "metadata": {},
   "source": [
    "### Google Gemini Env, Model, and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf211b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0390feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
    "\n",
    "output = model.invoke(\"hi\")\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3330b60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "embeddings.embed_query(\"Hello AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0249749",
   "metadata": {},
   "source": [
    "## Hugging Face Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c71d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN']=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
    "\n",
    "len(embeddings.embed_query(\"hi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330dd333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "text=\"this is atest documents\"\n",
    "query_result=embeddings.embed_query(text)\n",
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f800701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e437169",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ffd335",
   "metadata": {},
   "source": [
    "### WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# For 1 URL\n",
    "url = ''\n",
    "web_loader=WebBaseLoader(url)\n",
    "data=web_loader.load()\n",
    "\n",
    "# For Multi URL\n",
    "urls = ['', '']\n",
    "docs=[WebBaseLoader(url).load() for url in urls]\n",
    "docs_list=[item for sublist in docs for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf62c2",
   "metadata": {},
   "source": [
    "### TextLoader and DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ceef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "\n",
    "loader=DirectoryLoader(\"../data\",glob=\"./*.txt\",loader_cls=TextLoader)\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d817f9",
   "metadata": {},
   "source": [
    "### PDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc8d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader=PyPDFLoader('syllabus.pdf')\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11c68d",
   "metadata": {},
   "source": [
    "### ArXiv Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "docs = ArxivLoader(query=\"1706.03762\", load_max_docs=2).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d570177",
   "metadata": {},
   "source": [
    "### Wikipedia Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5868bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "docs = WikipediaLoader(query=\"Generative AI\", load_max_docs=4).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39272361",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90c597",
   "metadata": {},
   "source": [
    "### RecursiveCharaterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d012e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Normal Embedding Models\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# OpenAI Embedding Models\n",
    "text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder\n",
    "(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=25\n",
    ")\n",
    "\n",
    "# Common Code\n",
    "doc_splits=text_splitter.split_documents(docs_list)\n",
    "\n",
    "\n",
    "# If only page content needed\n",
    "doc_string=[doc.page_content for doc in doc_splits]\n",
    "\n",
    "# If need to preserve metadata\n",
    "texts = [doc.page_content for doc in doc_splits]\n",
    "metadatas = [doc.metadata for doc in doc_splits]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6e4bb7",
   "metadata": {},
   "source": [
    "### CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f7abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter=CharacterTextSplitter(separator=\"\\n\\n\",chunk_size=100,chunk_overlap=20)\n",
    "text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0173008",
   "metadata": {},
   "source": [
    "### HTMLHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e02bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "    <div>\n",
    "        <h1>Foo</h1>\n",
    "        <p>Some intro text about Foo.</p>\n",
    "        <div>\n",
    "            <h2>Bar main section</h2>\n",
    "            <p>Some intro text about Bar.</p>\n",
    "            <h3>Bar subsection 1</h3>\n",
    "            <p>Some text about the first subtopic of Bar.</p>\n",
    "            <h3>Bar subsection 2</h3>\n",
    "            <p>Some text about the second subtopic of Bar.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h2>Baz</h2>\n",
    "            <p>Some text about Baz</p>\n",
    "        </div>\n",
    "        <br>\n",
    "        <p>Some concluding text about Foo</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on=[\n",
    "    (\"h1\",\"Header 1\"),\n",
    "    (\"h2\",\"Header 2\"),\n",
    "    (\"h3\",\"Header 3\")\n",
    "]\n",
    "\n",
    "html_splitter=HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits=html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547b96f4",
   "metadata": {},
   "source": [
    "### RecursiveJsonSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d4f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "json_splitter=RecursiveJsonSplitter(max_chunk_size=300)\n",
    "json_chunks=json_splitter.split_json(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e7565",
   "metadata": {},
   "source": [
    "## Vector Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c01489",
   "metadata": {},
   "source": [
    "### FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad4427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "# Using Inner Product in FAISS Index\n",
    "\n",
    "index=faiss.IndexFlatIP(3072) # Number of dimensions in the embedding model\n",
    "\n",
    "db = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "# Using Euclidiean Distance in FAISS Index\n",
    "\n",
    "index=faiss.IndexFlatL2(384) # Number of dimensions in the embedding model\n",
    "\n",
    "db = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "# If we just need the docstrings\n",
    "db.add_texts(doc_string)\n",
    "\n",
    "# If we need to add metadata info as well\n",
    "db.add_texts(texts, metadatas=metadatas)\n",
    "\n",
    "# Note: Add texts only works with array of docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a30fa4",
   "metadata": {},
   "source": [
    "### Saving and Loading Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79467d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Index\n",
    "\n",
    "db.save_local(\"saved_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a090255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Index\n",
    "\n",
    "new_vector_store=FAISS.load_local(\n",
    "  \"saved_index\",\n",
    "  embeddings,\n",
    "  allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1318c",
   "metadata": {},
   "source": [
    "### Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore=Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chrome\", # Any Name\n",
    "    embedding=embeddings\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593aefa5",
   "metadata": {},
   "source": [
    "### Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a8f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pinecone_api_key=os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7996c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec  #Serverless: Server will be Managed by the cloud provider\n",
    "\n",
    "pc=Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Index Creation and Loading\n",
    "\n",
    "index_name=\"agentic-ai\"\n",
    "\n",
    "#creating a index\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=768,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(cloud=\"aws\",region=\"us-east-1\")    \n",
    ")\n",
    "\n",
    "#loading the index\n",
    "index=pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13002883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Vector Store and Similarity Search\n",
    "vector_store=PineconeVectorStore(index=index,embedding=embeddings)\n",
    "\n",
    "results = vector_store.similarity_search(\"what is a langchain?\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e99f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store Retriever\n",
    "\n",
    "retriever=vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.7} #hyperparameter\n",
    ")\n",
    "retriever.invoke(\"langchain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc8423",
   "metadata": {},
   "source": [
    "## Langchain Inbuilt Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb92c74",
   "metadata": {},
   "source": [
    "### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b23eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper=WikipediaAPIWrapper(top_k_results=5,doc_content_chars_max= 500)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper= api_wrapper)\n",
    "\n",
    "# To get tool name\n",
    "wiki_tool.name\n",
    "# To get tool description\n",
    "wiki_tool.description\n",
    "# To get tool args\n",
    "wiki_tool.args\n",
    "\n",
    "# Running\n",
    "wiki_tool.run({\"query\": \"elon musk\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46432b85",
   "metadata": {},
   "source": [
    "### Youtube Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4f5a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import YouTubeSearchTool\n",
    "\n",
    "tool = YouTubeSearchTool()\n",
    "\n",
    "# To get tool name\n",
    "tool.name\n",
    "# To get tool description\n",
    "tool.description\n",
    "# To get tool args\n",
    "tool.args\n",
    "\n",
    "# Running\n",
    "tool.run(\"Emergency Awesome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59299f25",
   "metadata": {},
   "source": [
    "### Tavily (Search Engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591921c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TAVILY_API_KEY=os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d382df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tool=TavilySearchResults(tavily_api_key=TAVILY_API_KEY)\n",
    "\n",
    "# Running - 1\n",
    "tool.invoke({\"query\":\"what happend between Trump and Musk today?\"})\n",
    "\n",
    "# Running - 2\n",
    "question = \"what happend between Trump and Musk today?\"\n",
    "complete_query = \"Anwer the follow question by searching the internet and getting best response. Following is the user question: \" + question\n",
    "\n",
    "tool.invoke(complete_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d10122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "tavily_tool=TavilySearch(tavily_api_key=TAVILY_API_KEY)\n",
    "\n",
    "question = \"what happend between Trump and Musk today?\"\n",
    "complete_query = \"Anwer the follow question by searching the internet and getting best response. Following is the user question: \" + question\n",
    "\n",
    "tavily_tool.invoke(complete_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a88ffe0",
   "metadata": {},
   "source": [
    "### DuckDuckGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d8424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "search.invoke(\"what is the latest update on iphone17 release?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be3b43",
   "metadata": {},
   "source": [
    "## Custom Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59822c6",
   "metadata": {},
   "source": [
    "### Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6908bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add two integers.\n",
    "\n",
    "    Args:\n",
    "    a(int): The first integer\n",
    "    b(int): The second integer\n",
    "\n",
    "    Returns:\n",
    "        int: The Sum of a and b\n",
    "    \"\"\"\n",
    "\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e38b8fa",
   "metadata": {},
   "source": [
    "### Subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ffc121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Subtract two integers.\n",
    "\n",
    "    Args:\n",
    "    a(int): The first integer\n",
    "    b(int): The second integer\n",
    "\n",
    "    Returns:\n",
    "        int: The difference of a and b\n",
    "    \"\"\"\n",
    "\n",
    "    return a - b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a1bbda",
   "metadata": {},
   "source": [
    "### Absolute Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f35cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def abs_diff(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Subtract two integers.\n",
    "\n",
    "    Args:\n",
    "    a(int): The first integer\n",
    "    b(int): The second integer\n",
    "\n",
    "    Returns:\n",
    "        int: The absolute difference of a and b\n",
    "    \"\"\"\n",
    "\n",
    "    return abs(a - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5b2318",
   "metadata": {},
   "source": [
    "### Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65aa43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiple(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Multiple two integers.\n",
    "\n",
    "    Args:\n",
    "    a(int): The first integer\n",
    "    b(int): The second integer\n",
    "\n",
    "    Returns:\n",
    "        int: The product of a and b\n",
    "    \"\"\"\n",
    "\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a6e9b",
   "metadata": {},
   "source": [
    "### Divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75d960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Divide two integers.\n",
    "\n",
    "    Args:\n",
    "    a(int): The first integer\n",
    "    b(int): The second integer\n",
    "\n",
    "    Returns:\n",
    "        int: The result of division\n",
    "    \"\"\"\n",
    "\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Denominator cannot be zero.\")\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f9fa7",
   "metadata": {},
   "source": [
    "### Length of Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_word_length(word:str)->int:\n",
    "    \"\"\"\n",
    "    Calculate the length of the word.\n",
    "\n",
    "    Args:\n",
    "    word(str): The word in string\n",
    "\n",
    "    Returns:\n",
    "        int: The length of the word\n",
    "    \"\"\"\n",
    "    return len(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c04654",
   "metadata": {},
   "source": [
    "## Agentic Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3816fa5f",
   "metadata": {},
   "source": [
    "### Pydantic Class for some kind of validation -> used as an Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel , Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "class TopicSelectionParser(BaseModel):\n",
    "    Topic:str=Field(description=\"selected topic\")\n",
    "    Reasoning:str=Field(description='Reasoning behind topic selection')\n",
    "\n",
    "parser=PydanticOutputParser(pydantic_object=TopicSelectionParser)\n",
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b31f5a6",
   "metadata": {},
   "source": [
    "### Custom Agent State Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f824a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc2ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "workflow = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aed967f",
   "metadata": {},
   "source": [
    "### Prebuilt Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16220d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState\n",
    "\n",
    "workflow = StateGraph(MessagesState) \n",
    "# This is the same as our custom defined Agent State Function (right now), if we need something custom, we can use our methods, else MessageState is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6aafff",
   "metadata": {},
   "source": [
    "### Workflow 1: Agentic Orchestration using parser(pydantic class), custom AgentState, and Custom Router Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef0e6d",
   "metadata": {},
   "source": [
    "![alt text](01a93893-3dce-4f90-80d0-b335c9bd36a2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# LLM Supervisor Function using Pydantic Parser, Custom Agent State, Chaining\n",
    "def llm_supervisor_function(state: AgentState):\n",
    "    question = state[\"messages\"][-1]\n",
    "\n",
    "    print(\"Question\", question)\n",
    "\n",
    "    template=\"\"\"\n",
    "    Your task is to classify the given user query into one of the following categories: [USA, Not Related]. \n",
    "    Only respond with the category name and nothing else.\n",
    "\n",
    "    User query: {question}\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"question\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions}\n",
    "    )\n",
    "\n",
    "    chain = prompt | model | parser\n",
    "\n",
    "    response = chain.invoke({\"question\": question})\n",
    "\n",
    "    print(\"Parsed response\", response)\n",
    "\n",
    "    return {\"messages\": [response.Topic]}\n",
    "\n",
    "# Custom Router Function\n",
    "def router_function(state: AgentState):\n",
    "    print(\"-> Router ->\")\n",
    "\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    print(\"last_message: \", last_message)\n",
    "\n",
    "    if \"usa\" in last_message.lower():\n",
    "        return \"RAG Call\"\n",
    "    else:\n",
    "        return \"LLM Call\"\n",
    "    \n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# RAG Function\n",
    "def function2(state: AgentState):\n",
    "    print(\"-> RAG Call ->\")\n",
    "    question = state[\"messages\"][0]\n",
    "    \n",
    "    prompt=PromptTemplate(\n",
    "        template = \"\"\"\n",
    "        You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "        If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\n",
    "        Question: {question} \\n\n",
    "        Context: {context} \\n\n",
    "        Answer:\n",
    "        \"\"\",\n",
    "        \n",
    "        input_variables=['context', 'question']\n",
    "    )\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    result = rag_chain.invoke(question)\n",
    "    return  {\"messages\": [result]}\n",
    "\n",
    "# LLM Function\n",
    "def function3(state: AgentState):\n",
    "    print(\"-> LLM Call ->\")\n",
    "    question = state[\"messages\"][0]\n",
    "    \n",
    "    # Normal LLM call\n",
    "    complete_query = \"Anwer the follow question with you knowledge of the real world. Following is the user question: \" + question\n",
    "    response = model.invoke(complete_query)\n",
    "    return {\"messages\": [response.content]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f5b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_node(\"Supervisor\", llm_supervisor_function)\n",
    "workflow.add_node(\"RAG\", function2)\n",
    "workflow.add_node(\"LLM\", function3)\n",
    "\n",
    "# One Way\n",
    "workflow.set_entry_point(\"Supervisor\")\n",
    "\n",
    "# Other Way\n",
    "workflow.add_edge(START, \"Supervisor\")\n",
    "\n",
    "# When Conditional Edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"Supervisor\",\n",
    "    router_function,\n",
    "    {\n",
    "        \"RAG Call\" : \"RAG\",\n",
    "        \"LLM Call\" : \"LLM\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"RAG\", END)\n",
    "workflow.add_edge(\"LLM\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861717c9",
   "metadata": {},
   "source": [
    "### Workflow 2: Agentic Orchestration Using Message State, Tool Node, Custom Tools, Multi Tool Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b70d9",
   "metadata": {},
   "source": [
    "![alt text](95f33936-ba48-4300-9742-488a7f1cd6f3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d561d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph,MessagesState,START,END\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cabf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Tool\n",
    "\n",
    "@tool\n",
    "def search(query:str):\n",
    "    \"\"\"this is my custom tool for searching a weather\"\"\"\n",
    "    if \"delhi\" in query.lower():\n",
    "        return \"the temp is 45 degree celsius\"\n",
    "    return \"the temp is 25 degree celsius\"\n",
    "\n",
    "# Binding Tool\n",
    "tools = [search]\n",
    "llm_with_tool = llm.bind_tools(tools)\n",
    "\n",
    "response = llm_with_tool.invoke(\"what is weather in delhi?\")\n",
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a94c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Message State\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    question = state['messages']\n",
    "    response = llm_with_tool.invoke(question)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Custom Router Function, which will be replaced by langgraph.prebuilt tools_condition \n",
    "def router_function(state: MessagesState):\n",
    "    message = state[\"messages\"]\n",
    "    last_message = message[-1]\n",
    "\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae2227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tool Node from Tool Node class\n",
    "\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0958a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow2 = StateGraph(MessagesState)\n",
    "workflow2.add_node(\"llmwithtool\",call_model)\n",
    "workflow2.add_node(\"mytools\", tool_node) # The tool Node is used here\n",
    "workflow2.add_edge(START, \"llmwithtool\")\n",
    "workflow2.add_conditional_edges(\"llmwithtool\",\n",
    "                                router_function,\n",
    "                                {\"tools\": \"mytools\",\n",
    "                                END: END})\n",
    "workflow2.add_edge(\"mytools\", \"llmwithtool\") # This edge makes multi tool call\n",
    "app2 = workflow2.compile()\n",
    "app2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e3fde4",
   "metadata": {},
   "source": [
    "### Using tools_condition instead of our custom Router function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using router_function in the add_condition_edges method, use tools_condition. This is a inbuilt function in langgraph which returns \"tools\" (same as in our router function)\n",
    "\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "workflow.add_conditional_edges(\"llmwithtool\",\n",
    "                            tools_condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d53158",
   "metadata": {},
   "source": [
    "### Use of Memory Saver, Stream Messages, and Pretty Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44220118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Only need to add checkpointer to this memory \n",
    "app2 = workflow2.compile(checkpointer=memory)\n",
    "app2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656366a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Creating this events \n",
    "events = app2.stream(\n",
    "    {\"messages\":[\"what is a weather in delhi can you tell me some good hotel for staying in north delhi\"]}, \n",
    "    config=config, \n",
    "    stream_mode=\"values\"\n",
    "    )\n",
    "\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc0a1b",
   "metadata": {},
   "source": [
    "### Viewing Compiled Workflow stored in app variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef35649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3320b661",
   "metadata": {},
   "source": [
    "### Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ff158",
   "metadata": {},
   "source": [
    "![alt text](004a619e-f184-41ff-b629-ecdad04b9eb0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0cffdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
