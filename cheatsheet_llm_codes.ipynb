{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a7f01e",
   "metadata": {},
   "source": [
    "## Loading Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bfb7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13fac10",
   "metadata": {},
   "source": [
    "## LLM APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698f92d8",
   "metadata": {},
   "source": [
    "### OpenAI Env, Model, and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9641b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dfc91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "llm.invoke(\"hello how are you my firend?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3db15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "len(embeddings.embed_query(\"hello how are you my firend?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353929d",
   "metadata": {},
   "source": [
    "### Groq KEY and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d73e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8630a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model_name=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "response=llm.invoke(\"what is length of wall of china?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0963e118",
   "metadata": {},
   "source": [
    "### Google Gemini Env, Model, and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf211b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0390feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
    "\n",
    "output = model.invoke(\"hi\")\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3330b60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "embeddings.embed_query(\"Hello AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0249749",
   "metadata": {},
   "source": [
    "## Hugging Face Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c71d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN']=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
    "\n",
    "len(embeddings.embed_query(\"hi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330dd333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "text=\"this is atest documents\"\n",
    "query_result=embeddings.embed_query(text)\n",
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f800701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e437169",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ffd335",
   "metadata": {},
   "source": [
    "### WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# For 1 URL\n",
    "url = ''\n",
    "web_loader=WebBaseLoader(url)\n",
    "data=web_loader.load()\n",
    "\n",
    "# For Multi URL\n",
    "urls = ['', '']\n",
    "docs=[WebBaseLoader(url).load() for url in urls]\n",
    "docs_list=[item for sublist in docs for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf62c2",
   "metadata": {},
   "source": [
    "### TextLoader and DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ceef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "\n",
    "loader=DirectoryLoader(\"../data\",glob=\"./*.txt\",loader_cls=TextLoader)\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d817f9",
   "metadata": {},
   "source": [
    "### PDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc8d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader=PyPDFLoader('syllabus.pdf')\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11c68d",
   "metadata": {},
   "source": [
    "### ArXiv Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "docs = ArxivLoader(query=\"1706.03762\", load_max_docs=2).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d570177",
   "metadata": {},
   "source": [
    "### Wikipedia Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5868bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "docs = WikipediaLoader(query=\"Generative AI\", load_max_docs=4).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39272361",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90c597",
   "metadata": {},
   "source": [
    "### RecursiveCharaterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d012e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Normal Embedding Models\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# OpenAI Embedding Models\n",
    "text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder\n",
    "(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=25\n",
    ")\n",
    "\n",
    "# Common Code\n",
    "doc_splits=text_splitter.split_documents(docs_list)\n",
    "\n",
    "\n",
    "# If only page content needed\n",
    "doc_string=[doc.page_content for doc in doc_splits]\n",
    "\n",
    "# If need to preserve metadata\n",
    "texts = [doc.page_content for doc in doc_splits]\n",
    "metadatas = [doc.metadata for doc in doc_splits]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6e4bb7",
   "metadata": {},
   "source": [
    "### CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f7abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter=CharacterTextSplitter(separator=\"\\n\\n\",chunk_size=100,chunk_overlap=20)\n",
    "text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0173008",
   "metadata": {},
   "source": [
    "### HTMLHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e02bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "    <div>\n",
    "        <h1>Foo</h1>\n",
    "        <p>Some intro text about Foo.</p>\n",
    "        <div>\n",
    "            <h2>Bar main section</h2>\n",
    "            <p>Some intro text about Bar.</p>\n",
    "            <h3>Bar subsection 1</h3>\n",
    "            <p>Some text about the first subtopic of Bar.</p>\n",
    "            <h3>Bar subsection 2</h3>\n",
    "            <p>Some text about the second subtopic of Bar.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h2>Baz</h2>\n",
    "            <p>Some text about Baz</p>\n",
    "        </div>\n",
    "        <br>\n",
    "        <p>Some concluding text about Foo</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on=[\n",
    "    (\"h1\",\"Header 1\"),\n",
    "    (\"h2\",\"Header 2\"),\n",
    "    (\"h3\",\"Header 3\")\n",
    "]\n",
    "\n",
    "html_splitter=HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits=html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547b96f4",
   "metadata": {},
   "source": [
    "### RecursiveJsonSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d4f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "json_splitter=RecursiveJsonSplitter(max_chunk_size=300)\n",
    "json_chunks=json_splitter.split_json(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e7565",
   "metadata": {},
   "source": [
    "## Vector Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c01489",
   "metadata": {},
   "source": [
    "### FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad4427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "# Using Inner Product in FAISS Index\n",
    "\n",
    "index=faiss.IndexFlatIP(3072) # Number of dimensions in the embedding model\n",
    "\n",
    "db = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "# Using Euclidiean Distance in FAISS Index\n",
    "\n",
    "index=faiss.IndexFlatL2(384) # Number of dimensions in the embedding model\n",
    "\n",
    "db = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "# If we just need the docstrings\n",
    "db.add_texts(doc_string)\n",
    "\n",
    "# If we need to add metadata info as well\n",
    "db.add_texts(texts, metadatas=metadatas)\n",
    "\n",
    "# Note: Add texts only works with array of docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a30fa4",
   "metadata": {},
   "source": [
    "### Saving and Loading Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79467d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Index\n",
    "\n",
    "db.save_local(\"saved_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a090255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Index\n",
    "\n",
    "new_vector_store=FAISS.load_local(\n",
    "  \"saved_index\",\n",
    "  embeddings,\n",
    "  allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1318c",
   "metadata": {},
   "source": [
    "### Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore=Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chrome\", # Any Name\n",
    "    embedding=embeddings\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593aefa5",
   "metadata": {},
   "source": [
    "### Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a8f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pinecone_api_key=os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7996c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec  #Serverless: Server will be Managed by the cloud provider\n",
    "\n",
    "pc=Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Index Creation and Loading\n",
    "\n",
    "index_name=\"agentic-ai\"\n",
    "\n",
    "#creating a index\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=768,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(cloud=\"aws\",region=\"us-east-1\")    \n",
    ")\n",
    "\n",
    "#loading the index\n",
    "index=pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13002883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Vector Store and Similarity Search\n",
    "vector_store=PineconeVectorStore(index=index,embedding=embeddings)\n",
    "\n",
    "results = vector_store.similarity_search(\"what is a langchain?\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e99f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store Retriever\n",
    "\n",
    "retriever=vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.7} #hyperparameter\n",
    ")\n",
    "retriever.invoke(\"langchain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc8423",
   "metadata": {},
   "source": [
    "## Langchain Inbuilt Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb92c74",
   "metadata": {},
   "source": [
    "### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b23eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper=WikipediaAPIWrapper(top_k_results=5,doc_content_chars_max= 500)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper= api_wrapper)\n",
    "\n",
    "# To get tool name\n",
    "wiki_tool.name\n",
    "# To get tool description\n",
    "wiki_tool.description\n",
    "# To get tool args\n",
    "wiki_tool.args\n",
    "\n",
    "# Running\n",
    "wiki_tool.run({\"query\": \"elon musk\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46432b85",
   "metadata": {},
   "source": [
    "### Youtube Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4f5a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import YouTubeSearchTool\n",
    "\n",
    "tool = YouTubeSearchTool()\n",
    "\n",
    "# To get tool name\n",
    "tool.name\n",
    "# To get tool description\n",
    "tool.description\n",
    "# To get tool args\n",
    "tool.args\n",
    "\n",
    "# Running\n",
    "tool.run(\"Emergency Awesome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59299f25",
   "metadata": {},
   "source": [
    "### Tavily (Search Engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591921c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TAVILY_API_KEY=os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d382df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tool=TavilySearchResults(tavily_api_key=TAVILY_API_KEY)\n",
    "\n",
    "# Running - 1\n",
    "tool.invoke({\"query\":\"what happend between Trump and Musk today?\"})\n",
    "\n",
    "# Running - 2\n",
    "question = \"what happend between Trump and Musk today?\"\n",
    "complete_query = \"Anwer the follow question by searching the internet and getting best response. Following is the user question: \" + question\n",
    "\n",
    "tool.invoke(complete_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d10122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "tavily_tool=TavilySearch(tavily_api_key=TAVILY_API_KEY)\n",
    "\n",
    "question = \"what happend between Trump and Musk today?\"\n",
    "complete_query = \"Anwer the follow question by searching the internet and getting best response. Following is the user question: \" + question\n",
    "\n",
    "tavily_tool.invoke(complete_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a88ffe0",
   "metadata": {},
   "source": [
    "### DuckDuckGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d8424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "search.invoke(\"what is the latest update on iphone17 release?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e29e3e",
   "metadata": {},
   "source": [
    "### Python REPL Utililty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b180b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run any given python code\n",
    "\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "code = \"\"\"\n",
    "x = 5\n",
    "y = x * 2\n",
    "print(y)\n",
    "\"\"\"\n",
    "\n",
    "repl.run(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be3b43",
   "metadata": {},
   "source": [
    "## Custom Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59822c6",
   "metadata": {},
   "source": [
    "### Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6908bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add two integers.\n",
    "\n",
    "    Args:\n",
    "    a(int): The first integer\n",
    "    b(int): The second integer\n",
    "\n",
    "    Returns:\n",
    "        int: The Sum of a and b\n",
    "    \"\"\"\n",
    "\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e38b8fa",
   "metadata": {},
   "source": [
    "### Subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ffc121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Subtract two integers.\n",
    "\n",
    "    Args:\n",
    "    a(int): The first integer\n",
    "    b(int): The second integer\n",
    "\n",
    "    Returns:\n",
    "        int: The difference of a and b\n",
    "    \"\"\"\n",
    "\n",
    "    return a - b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a1bbda",
   "metadata": {},
   "source": [
    "### Absolute Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f35cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def abs_diff(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Subtract two integers.\n",
    "\n",
    "    Args:\n",
    "    a(int): The first integer\n",
    "    b(int): The second integer\n",
    "\n",
    "    Returns:\n",
    "        int: The absolute difference of a and b\n",
    "    \"\"\"\n",
    "\n",
    "    return abs(a - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5b2318",
   "metadata": {},
   "source": [
    "### Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65aa43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiple(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Multiple two integers.\n",
    "\n",
    "    Args:\n",
    "    a(int): The first integer\n",
    "    b(int): The second integer\n",
    "\n",
    "    Returns:\n",
    "        int: The product of a and b\n",
    "    \"\"\"\n",
    "\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a6e9b",
   "metadata": {},
   "source": [
    "### Divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75d960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Divide two integers.\n",
    "\n",
    "    Args:\n",
    "    a(int): The first integer\n",
    "    b(int): The second integer\n",
    "\n",
    "    Returns:\n",
    "        int: The result of division\n",
    "    \"\"\"\n",
    "\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Denominator cannot be zero.\")\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f9fa7",
   "metadata": {},
   "source": [
    "### Length of Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_word_length(word:str)->int:\n",
    "    \"\"\"\n",
    "    Calculate the length of the word.\n",
    "\n",
    "    Args:\n",
    "    word(str): The word in string\n",
    "\n",
    "    Returns:\n",
    "        int: The length of the word\n",
    "    \"\"\"\n",
    "    return len(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c04654",
   "metadata": {},
   "source": [
    "## Agentic Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3816fa5f",
   "metadata": {},
   "source": [
    "### Pydantic Class for some kind of validation -> used as an Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel , Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "class TopicSelectionParser(BaseModel):\n",
    "    Topic:str=Field(description=\"selected topic\")\n",
    "    Reasoning:str=Field(description='Reasoning behind topic selection')\n",
    "\n",
    "parser=PydanticOutputParser(pydantic_object=TopicSelectionParser)\n",
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b31f5a6",
   "metadata": {},
   "source": [
    "### Custom Agent State Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f824a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc2ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "workflow = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aed967f",
   "metadata": {},
   "source": [
    "### Prebuilt Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16220d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState\n",
    "\n",
    "workflow = StateGraph(MessagesState) \n",
    "# This is the same as our custom defined Agent State Function (right now), if we need something custom, we can use our methods, else MessageState is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6aafff",
   "metadata": {},
   "source": [
    "### Workflow 1: Agentic Orchestration using parser(pydantic class), custom AgentState, and Custom Router Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef0e6d",
   "metadata": {},
   "source": [
    "![alt text](01a93893-3dce-4f90-80d0-b335c9bd36a2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# LLM Supervisor Function using Pydantic Parser, Custom Agent State, Chaining\n",
    "def llm_supervisor_function(state: AgentState):\n",
    "    question = state[\"messages\"][-1]\n",
    "\n",
    "    print(\"Question\", question)\n",
    "\n",
    "    template=\"\"\"\n",
    "    Your task is to classify the given user query into one of the following categories: [USA, Not Related]. \n",
    "    Only respond with the category name and nothing else.\n",
    "\n",
    "    User query: {question}\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"question\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions}\n",
    "    )\n",
    "\n",
    "    chain = prompt | model | parser\n",
    "\n",
    "    response = chain.invoke({\"question\": question})\n",
    "\n",
    "    print(\"Parsed response\", response)\n",
    "\n",
    "    return {\"messages\": [response.Topic]}\n",
    "\n",
    "# Custom Router Function\n",
    "def router_function(state: AgentState):\n",
    "    print(\"-> Router ->\")\n",
    "\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    print(\"last_message: \", last_message)\n",
    "\n",
    "    if \"usa\" in last_message.lower():\n",
    "        return \"RAG Call\"\n",
    "    else:\n",
    "        return \"LLM Call\"\n",
    "    \n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# RAG Function\n",
    "def function2(state: AgentState):\n",
    "    print(\"-> RAG Call ->\")\n",
    "    question = state[\"messages\"][0]\n",
    "    \n",
    "    prompt=PromptTemplate(\n",
    "        template = \"\"\"\n",
    "        You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "        If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\n",
    "        Question: {question} \\n\n",
    "        Context: {context} \\n\n",
    "        Answer:\n",
    "        \"\"\",\n",
    "        \n",
    "        input_variables=['context', 'question']\n",
    "    )\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    result = rag_chain.invoke(question)\n",
    "    return  {\"messages\": [result]}\n",
    "\n",
    "# LLM Function\n",
    "def function3(state: AgentState):\n",
    "    print(\"-> LLM Call ->\")\n",
    "    question = state[\"messages\"][0]\n",
    "    \n",
    "    # Normal LLM call\n",
    "    complete_query = \"Anwer the follow question with you knowledge of the real world. Following is the user question: \" + question\n",
    "    response = model.invoke(complete_query)\n",
    "    return {\"messages\": [response.content]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f5b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_node(\"Supervisor\", llm_supervisor_function)\n",
    "workflow.add_node(\"RAG\", function2)\n",
    "workflow.add_node(\"LLM\", function3)\n",
    "\n",
    "# One Way\n",
    "workflow.set_entry_point(\"Supervisor\")\n",
    "\n",
    "# Other Way\n",
    "workflow.add_edge(START, \"Supervisor\")\n",
    "\n",
    "# When Conditional Edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"Supervisor\",\n",
    "    router_function,\n",
    "    {\n",
    "        \"RAG Call\" : \"RAG\",\n",
    "        \"LLM Call\" : \"LLM\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"RAG\", END)\n",
    "workflow.add_edge(\"LLM\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861717c9",
   "metadata": {},
   "source": [
    "### Workflow 2: Agentic Orchestration Using Message State, Tool Node, Custom Tools, Multi Tool Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b70d9",
   "metadata": {},
   "source": [
    "![alt text](95f33936-ba48-4300-9742-488a7f1cd6f3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d561d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph,MessagesState,START,END\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cabf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Tool\n",
    "\n",
    "@tool\n",
    "def search(query:str):\n",
    "    \"\"\"this is my custom tool for searching a weather\"\"\"\n",
    "    if \"delhi\" in query.lower():\n",
    "        return \"the temp is 45 degree celsius\"\n",
    "    return \"the temp is 25 degree celsius\"\n",
    "\n",
    "# Binding Tool\n",
    "tools = [search]\n",
    "llm_with_tool = llm.bind_tools(tools)\n",
    "\n",
    "response = llm_with_tool.invoke(\"what is weather in delhi?\")\n",
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a94c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Message State\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    question = state['messages']\n",
    "    response = llm_with_tool.invoke(question)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Custom Router Function, which will be replaced by langgraph.prebuilt tools_condition \n",
    "def router_function(state: MessagesState):\n",
    "    message = state[\"messages\"]\n",
    "    last_message = message[-1]\n",
    "\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae2227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tool Node from Tool Node class\n",
    "\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0958a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow2 = StateGraph(MessagesState)\n",
    "workflow2.add_node(\"llmwithtool\",call_model)\n",
    "workflow2.add_node(\"mytools\", tool_node) # The tool Node is used here\n",
    "workflow2.add_edge(START, \"llmwithtool\")\n",
    "workflow2.add_conditional_edges(\"llmwithtool\",\n",
    "                                router_function,\n",
    "                                {\"tools\": \"mytools\",\n",
    "                                END: END})\n",
    "workflow2.add_edge(\"mytools\", \"llmwithtool\") # This edge makes multi tool call\n",
    "app2 = workflow2.compile()\n",
    "app2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e3fde4",
   "metadata": {},
   "source": [
    "### Using tools_condition instead of our custom Router function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using router_function in the add_condition_edges method, use tools_condition. This is a inbuilt function in langgraph which returns \"tools\" (same as in our router function)\n",
    "\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "workflow.add_conditional_edges(\"llmwithtool\",\n",
    "                            tools_condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d53158",
   "metadata": {},
   "source": [
    "### Use of Memory Saver, Stream Messages, and Pretty Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44220118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Only need to add checkpointer to this memory \n",
    "app2 = workflow2.compile(checkpointer=memory)\n",
    "app2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656366a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Creating this events \n",
    "events = app2.stream(\n",
    "    {\"messages\":[\"what is a weather in delhi can you tell me some good hotel for staying in north delhi\"]}, \n",
    "    config=config, \n",
    "    stream_mode=\"values\"\n",
    "    )\n",
    "\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc0a1b",
   "metadata": {},
   "source": [
    "### Viewing Compiled Workflow stored in app variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef35649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3320b661",
   "metadata": {},
   "source": [
    "### Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ff158",
   "metadata": {},
   "source": [
    "![alt text](004a619e-f184-41ff-b629-ecdad04b9eb0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0cffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Retrieval Step until Vector Embedding creation and adding the doc list to it\n",
    "\n",
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0873d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Adding retriever as a tool\n",
    "retriever_tool=create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retriever_blog_post\", # Any Name\n",
    "    \"vector database data description, comprehensive for the model to make sense of\", # Description for vector db data\n",
    "    )\n",
    "\n",
    "# Adding to tools and Tool Node Creation\n",
    "tools=[retriever_tool]\n",
    "llm_with_tool=llm.bind_tools(tools)\n",
    "\n",
    "retriever_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94695de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated,Sequence, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "# Pydantic Class \n",
    "class grade(BaseModel):\n",
    "    binary_score:str=Field(description=\"Relvance score 'yes' or 'no'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c692937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "from langchain import hub\n",
    "from typing import Literal\n",
    "#we use it for type of hinting\n",
    "\n",
    "def LLM_Decision_Maker(state:AgentState):\n",
    "    print(\"----CALL LLM_DECISION_MAKE----\")\n",
    "    message=state[\"messages\"]\n",
    "    last_message=message[-1]\n",
    "    question=last_message.content\n",
    "    response=llm_with_tool.invoke(question)\n",
    "    return {\"messages\":[response]}\n",
    "\n",
    "\n",
    "def grade_documents(state:AgentState)->Literal[\"Output Generator\", \"Query Rewriter\"]:\n",
    "    print(\"----CALLING GRADE FOR CHECKING RELEVANCY----\")\n",
    "    llm_with_structure_op=llm.with_structured_output(grade) # To provide response in a certain assigned schema (grade here)\n",
    "    \n",
    "    prompt=PromptTemplate(\n",
    "        template=\"\"\"You are a grader deciding if a document is relevant to a user’s question.\n",
    "                    Here is the document: {context}\n",
    "                    Here is the user’s question: {question}\n",
    "                    If the document talks about or contains information related to the user’s question, mark it as relevant. \n",
    "                    Give a 'yes' or 'no' answer to show if the document is relevant to the question.\"\"\",\n",
    "                    input_variables=[\"context\", \"question\"]\n",
    "                    )\n",
    "     \n",
    "    chain=prompt|llm_with_structure_op\n",
    "     \n",
    "     \n",
    "    message=state['messages']\n",
    "    \n",
    "    last_message = message[-1]\n",
    "    \n",
    "    question = message[0].content\n",
    "    \n",
    "    docs = last_message.content\n",
    "    \n",
    "    scored_result=chain.invoke({\"question\": question, \"context\": docs})\n",
    "    \n",
    "    score=scored_result.binary_score\n",
    "     \n",
    "    if score==\"yes\":\n",
    "        print(\"----DECISION: DOCS ARE RELEVANT----\")\n",
    "        return \"generator\"\n",
    "    else:\n",
    "        print(\"----DECISION: DOCS ARE NOT RELEVANT----\")\n",
    "        return \"rewriter\"\n",
    "    \n",
    "\n",
    "def generate(state:AgentState):\n",
    "    print(\"----RAG OUTPUT GENERATE----\")\n",
    "    \n",
    "    message=state[\"messages\"]\n",
    "    question=message[0].content\n",
    "    \n",
    "    last_message = message[-1]\n",
    "    docs = last_message.content\n",
    "\n",
    "    # To assign a said prompt for the RAG part of code using the preexisting library\n",
    "    prompt=hub.pull(\"rlm/rag-prompt\")\n",
    "    \n",
    "    rag_chain=prompt | llm\n",
    "    \n",
    "    response=rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    \n",
    "    print(f\"this is my response:{response}\")\n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def rewrite(state:AgentState):\n",
    "    print(\"----TRANSFORM QUERY----\")\n",
    "    message=state[\"messages\"]\n",
    "    \n",
    "    question=message[0].content\n",
    "    \n",
    "    input= [HumanMessage(content=f\"\"\"Look at the input and try to reason about the underlying semantic intent or meaning. \n",
    "                    Here is the initial question: {question} \n",
    "                    Formulate an improved question: \"\"\")\n",
    "       ]\n",
    "\n",
    "    response=llm.invoke(input)\n",
    "    \n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdaa757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agentic Orchestration\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "workflow=StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"LLM Decision Maker\",LLM_Decision_Maker)\n",
    "workflow.add_node(\"Vector Retriever\",retriever_node)\n",
    "workflow.add_node(\"Output Generator\",generate)\n",
    "workflow.add_node(\"Query Rewriter\",rewrite)\n",
    "\n",
    "workflow.add_edge(START,\"LLM Decision Maker\")\n",
    "workflow.add_conditional_edges(\"LLM Decision Maker\",\n",
    "                               tools_condition,\n",
    "                               {\"tools\":\"Vector Retriever\",\n",
    "                                END:END\n",
    "                                })\n",
    "workflow.add_conditional_edges(\"Vector Retriever\",\n",
    "                               grade_documents,\n",
    "                               {\"generator\":\"Output Generator\",\n",
    "                                \"rewriter\":\"Query Rewriter\"\n",
    "                                })\n",
    "workflow.add_edge(\"Output Generator\",END)\n",
    "workflow.add_edge(\"Query Rewriter\",\"LLM Decision Maker\")\n",
    "\n",
    "app=workflow.compile()\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2b2c6",
   "metadata": {},
   "source": [
    "### Invoking created Agentic Apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b70263",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.invoke({\"messages\":[\"what is LLM Powered Autonomous Agents explain the planning and reflection and prompt engineering explain me in terms of agents and langchain?\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc603c2",
   "metadata": {},
   "source": [
    "## MultiAgents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3f1afb",
   "metadata": {},
   "source": [
    "#### Concept of Command in MultiAgents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c7c9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition is 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Command(update={'sum': 30}, goto='multiply')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.types import Command\n",
    "\n",
    "def add_number(state):\n",
    "    result = state[\"num1\"] + state[\"num2\"]\n",
    "    print(f\"Addition is {result}\")\n",
    "\n",
    "    return Command(goto=\"multiply\", update = {\"sum\": result}) # Updates the current state, and tells which Agent to go to Next\n",
    "\n",
    "state = {\"num1\": 10, \"num2\": 20}\n",
    "\n",
    "add_number(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c8a88",
   "metadata": {},
   "source": [
    "#### Concept of create_react_agent to use inbuilt library to create an agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dae8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Creates an Agent, with defined LLM, tools, and the PROMPT we provide\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "        llm,\n",
    "        tools = [tools_arr],\n",
    "        prompt = \"PROMPT FOR THE AGENT\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3561c9d",
   "metadata": {},
   "source": [
    "### Network/Collaboration MultiAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111cdfc",
   "metadata": {},
   "source": [
    "![alt text](b9c5dea6-b44a-4c74-a92f-9d2642bbd17b.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3420e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "from typing import Annotated\n",
    "from langgraph.types import Command\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import MessagesState,StateGraph, START,END\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ea2e3",
   "metadata": {},
   "source": [
    "#### Tools and Function Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da31808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python REPL Tool Creation to run the code it recieves\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "    ):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "        \n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    \n",
    "    result_str = f\"Successfully executed:\\n\\`\\`\\`python\\n{code}\\n\\`\\`\\`\\nStdout: {result}\"\n",
    "    return (\n",
    "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# A system prompt to tell LLM, adding agent vise context\n",
    "def make_system_prompt(instruction:str)->str:\n",
    "    return  (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "        \" will help where you left off. Execute what you can to make progress.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "        f\"\\n{instruction}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Directing the LLM, what to do based on the last_message\n",
    "def get_next_node(last_message: BaseMessage, goto: str):\n",
    "    if \"FINAL ANSWER\" in last_message:\n",
    "        return END\n",
    "    \n",
    "    return goto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a447e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 1 -> Researcher Node\n",
    "\n",
    "def research_node(state: MessagesState)-> Command[Literal[\"chart_generator\", END]]:\n",
    "    research_agent = create_react_agent(\n",
    "        llm,\n",
    "        tools = [search_tool],\n",
    "        prompt = make_system_prompt(\n",
    "            \"You can only do research. You are working with a chart generator colleague.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    result = research_agent.invoke(state)\n",
    "\n",
    "    last_message = result[\"messages\"][-1]\n",
    "\n",
    "    goto=get_next_node(last_message,\"chart_generator\")\n",
    "\n",
    "    result[\"messages\"][-1] = HumanMessage(content=result[\"messages\"][-1].content, name=\"researcher\")\n",
    "\n",
    "    return Command(update= {\"messages\": result[\"messages\"]}, goto = goto)\n",
    "\n",
    "\n",
    "# Agent 2 -> Chart Creation Node\n",
    "\n",
    "def chart_node(state: MessagesState)-> Command[Literal[\"researcher\", END]]:\n",
    "    chart_agent = create_react_agent(\n",
    "        llm,\n",
    "        tools = [python_repl_tool],\n",
    "        prompt = make_system_prompt(\n",
    "            \"You can only generate charts. You are working with a researcher colleague.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    result = chart_agent.invoke(state)\n",
    "\n",
    "    last_message = result[\"messages\"][-1]\n",
    "\n",
    "    goto=get_next_node(last_message,\"researcher\")\n",
    "\n",
    "    result[\"messages\"][-1] = HumanMessage(content=result[\"messages\"][-1].content, name=\"chart_generator\")\n",
    "\n",
    "    return Command(update= {\"messages\": result[\"messages\"]}, goto= goto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e6418b",
   "metadata": {},
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ed8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"researcher\", research_node)\n",
    "workflow.add_node(\"chart_generator\", chart_node)\n",
    "\n",
    "workflow.add_edge(START, \"researcher\")\n",
    "app = workflow.compile()\n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af30fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.invoke({\"messages\": [(\"user\",\"get the UK's GDP over the past 3 years, then make a line chart of it.Once you make the chart, finish.\")],})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d77e476",
   "metadata": {},
   "source": [
    "### Supervisor MultiAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27988e9",
   "metadata": {},
   "source": [
    "![alt text](a3a4db77-825b-49e9-abdc-ff0bb7255e37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e52e78",
   "metadata": {},
   "source": [
    "#### Tools and Function Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a81a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from typing import Annotated\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3045ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool 1 - Researcher\n",
    "TAVILY_API_KEY=os.getenv(\"TAVILY_API_KEY\")\n",
    "search_tool=TavilySearchResults(tavily_api_key=TAVILY_API_KEY)\n",
    "\n",
    "\n",
    "# Tool 2 - Coder\n",
    "repl=PythonREPL()\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(code: Annotated[str, \"The python code to execute to generate your chart.\"]):\n",
    "    \"\"\"Use this to execute python code and do math. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    \n",
    "    result_str = f\"Successfully executed:\\n\\`\\`\\`python\\n{code}\\n\\`\\`\\`\\nStdout: {result}\"\n",
    "    return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c34550",
   "metadata": {},
   "outputs": [],
   "source": [
    "members=[\"researcher\",\"coder\"]\n",
    "\n",
    "options = members+[\"FINISH\"]\n",
    "\n",
    "class Router(TypedDict):\n",
    "    next: Literal['researcher', 'coder', 'FINISH']\n",
    "\n",
    "#### IMPORTANT\n",
    "class State(MessagesState):\n",
    "    next: str\n",
    "\n",
    "# State will look like this\n",
    "# state={\"messages\": [\"hi\"], \"next\": \"research_agent\"}\n",
    "\n",
    "system_prompt = f\"\"\"\"\n",
    "You are a supervisor, tasked with managing a conversation between the following workers: {members}. \n",
    "Given the following user request, respond with the worker to act next. \n",
    "Each worker will perform a task and respond with their results and status. \n",
    "When finished, respond with FINISH.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba27d686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGENT 1 [SUPERVISOR]\n",
    "\n",
    "def supervisor_agent(state: State)->Command[Literal['researcher', 'coder', '__end__']]:\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}, ] + state[\"messages\"]\n",
    "\n",
    "    llm_with_structured_output = llm.with_structured_output(Router)\n",
    "\n",
    "    response = llm_with_structured_output.invoke(messages)\n",
    "\n",
    "    # this is my next worker agent\n",
    "    goto = response[\"next\"]\n",
    "\n",
    "    print(\"***************BELOW IS MY GOTO****************\")\n",
    "\n",
    "    print(goto)\n",
    "\n",
    "    if goto == \"FINISH\":\n",
    "        return '__end__'\n",
    "    \n",
    "    return Command(goto= goto, update={\"next\": goto})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a2cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGENT 2 [RESEARCHER]\n",
    "\n",
    "def researcher_agent(state: State)->Command[Literal['supervisor']]:\n",
    "    research_agent = create_react_agent(llm, tools=[search_tool], prompt=\"You are a researcher. DO NOT do any math.\")\n",
    "\n",
    "    result = research_agent.invoke(state)\n",
    "\n",
    "    return Command(\n",
    "        update = {\n",
    "            \"messages\": [\n",
    "                HumanMessage(content = result[\"messages\"][-1].content, name=\"researcher\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6808bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGENT 3 [CODER]\n",
    "\n",
    "def coder_agent(state: State)->Command[Literal['supervisor']]:\n",
    "    code_agent = create_react_agent(llm, tools=[python_repl_tool], prompt=\"You are a coder. DO NOT do any research.\")\n",
    "\n",
    "    result = code_agent.invoke(state)\n",
    "\n",
    "    return Command(\n",
    "        update = {\n",
    "            \"messages\": [\n",
    "                HumanMessage(content = result[\"messages\"][-1].content, name = \"coder\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0504c771",
   "metadata": {},
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed37d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(State)\n",
    "\n",
    "graph.add_node(\"supervisor\", supervisor_agent)\n",
    "graph.add_node(\"researcher\", researcher_agent)\n",
    "graph.add_node(\"coder\", coder_agent)\n",
    "\n",
    "graph.add_edge(START, \"supervisor\")\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb453d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in app.stream({\"messages\": [(\"user\", \"What's the square root of 49?\")]}, subgraphs=True):\n",
    "    print(s)\n",
    "    print(\"**********BELOW IS MY STATE***************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a785b367",
   "metadata": {},
   "source": [
    "### Human In Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfafea75",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
